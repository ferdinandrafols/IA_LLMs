{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdinandrafols/IA_LLMs/blob/main/gsi073_aula0_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepara√ß√£o dos dados\n",
        "\n",
        "Esta tarefa √© inverter sequ√™ncias de caracteres. Exemplo: **aabcd** em **dcbaa**.\n"
      ],
      "metadata": {
        "id": "o8ZBsZcshFv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# ===== 1. Dicion√°rio e fun√ß√µes b√°sicas =====\n",
        "chars = list(\"abcde \")  # Define o conjunto de caracteres permitidos (a-e e espa√ßo)\n",
        "vocab = {ch: i for i, ch in enumerate(chars)}  # Mapeia cada caractere para um √≠ndice num√©rico\n",
        "inv_vocab = {i: ch for ch, i in vocab.items()} # Cria um dicion√°rio inverso para decodificar √≠ndices para caracteres\n",
        "vocab_size = len(vocab)  # Quantidade total de tokens poss√≠veis\n",
        "\n",
        "def encode(s):  # Converte uma string em uma sequ√™ncia de √≠ndices num√©ricos\n",
        "    return torch.tensor([vocab[c] for c in s], dtype=torch.long)\n",
        "\n",
        "def decode(t):  # Converte uma sequ√™ncia de √≠ndices num√©ricos de volta para string\n",
        "    return ''.join(inv_vocab[int(x)] for x in t)\n",
        "\n",
        "def random_seq(n=5):  # Gera uma sequ√™ncia aleat√≥ria de tamanho n usando apenas 'abcde'\n",
        "    return ''.join(random.choice(chars[:-1]) for _ in range(n))  # Exclui o espa√ßo\n",
        "\n",
        "# ===== 2. Gerar dados =====\n",
        "pairs = [(encode(s), encode(s[::-1])) for s in [random_seq() for _ in range(50000)]]  # Cria 50k pares (sequ√™ncia, sequ√™ncia invertida)\n",
        "max_len = max(len(x) for x, _ in pairs)  # Descobre o maior comprimento de sequ√™ncia\n",
        "\n",
        "def pad(x):  # Preenche a sequ√™ncia com espa√ßos para padronizar o tamanho\n",
        "    return torch.cat([x, torch.tensor([vocab[' ']] * (max_len - len(x)))], dim=0)\n",
        "\n",
        "inputs = torch.stack([pad(x) for x, _ in pairs])  # Aplica padding em todas as entradas\n",
        "targets = torch.stack([pad(y) for _, y in pairs])  # Aplica padding em todos os alvos\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(inputs, targets)  # Cria dataset PyTorch com entradas e alvos\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)  # Cria DataLoader com batch de 128\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Seleciona GPU se dispon√≠vel\n",
        "\n",
        "# ===== 3. Prints para inspecionar =====\n",
        "print(f\"Vocabul√°rio: {vocab}\")  # Mostra o dicion√°rio de tokens\n",
        "print(f\"Tamanho do vocabul√°rio: {vocab_size}\")  # Mostra quantos tokens existem\n",
        "print(f\"Tamanho m√°ximo das sequ√™ncias (max_len): {max_len}\")  # Mostra o comprimento m√°ximo\n",
        "\n",
        "# Mostrar 3 exemplos codificados/decodificados\n",
        "for i in range(3):\n",
        "    s = random_seq()  # Gera nova sequ√™ncia aleat√≥ria\n",
        "    encoded = encode(s)  # Codifica para √≠ndices\n",
        "    decoded = decode(encoded)  # Decodifica de volta\n",
        "    reversed_decoded = decode(encoded.flip(0))  # Inverte e decodifica (target esperado)\n",
        "    print(f\"\\nExemplo {i+1}:\")\n",
        "    print(f\"  Original: {s}\")\n",
        "    print(f\"  Codificado: {encoded.tolist()}\")\n",
        "    print(f\"  Decodificado: {decoded}\")\n",
        "    print(f\"  Reverso (target esperado): {reversed_decoded}\")\n",
        "\n",
        "# Mostrar formas (shapes) dos tensores de entrada e sa√≠da\n",
        "print(\"\\nShapes:\")\n",
        "print(f\"  inputs:  {inputs.shape}\")  # Dimens√£o das entradas\n",
        "print(f\"  targets: {targets.shape}\")  # Dimens√£o dos alvos\n",
        "\n",
        "# Mostrar o primeiro batch do DataLoader\n",
        "for xb, yb in train_dl:\n",
        "    print(\"\\nPrimeiro batch de treino:\")\n",
        "    print(\"  Entradas (xb):\", xb.shape)  # Mostra tamanho do batch\n",
        "    print(\"  Alvos (yb):\", yb.shape)  # Mostra tamanho dos alvos\n",
        "    print(\"  Exemplo de entrada decodificada:\", decode(xb[0]))  # Converte o primeiro exemplo do batch em string\n",
        "    print(\"  Exemplo de alvo decodificado:\", decode(yb[0]))  # Converte o alvo correspondente\n",
        "    break  # Mostra apenas o primeiro batch\n"
      ],
      "metadata": {
        "id": "ylarIE4Wd0ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabul√°rio: {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, ' ': 5}\n",
        "Tamanho do vocabul√°rio: 6\n",
        "Tamanho m√°ximo das sequ√™ncias (max_len): 5\n",
        "\n",
        "Exemplo 1:\n",
        "  Original: deeec\n",
        "  Codificado: [3, 4, 4, 4, 2]\n",
        "  Decodificado: deeec\n",
        "  Reverso (target esperado): ceeed\n",
        "\n",
        "Exemplo 2:\n",
        "  Original: cebcc\n",
        "  Codificado: [2, 4, 1, 2, 2]\n",
        "  Decodificado: cebcc\n",
        "  Reverso (target esperado): ccbec\n",
        "\n",
        "Exemplo 3:\n",
        "  Original: ceaea\n",
        "  Codificado: [2, 4, 0, 4, 0]\n",
        "  Decodificado: ceaea\n",
        "  Reverso (target esperado): aeaec\n",
        "\n",
        "Shapes:\n",
        "  inputs:  torch.Size([50000, 5])\n",
        "  targets: torch.Size([50000, 5])\n",
        "\n",
        "Primeiro batch de treino:\n",
        "  Entradas (xb): torch.Size([128, 5])\n",
        "  Alvos (yb): torch.Size([128, 5])\n",
        "  Exemplo de entrada decodificada: eebde\n",
        "  Exemplo de alvo decodificado: edbee\n",
        "\n",
        "Excelente ‚Äî esse output mostra que voc√™ **compreendeu e reproduziu um pipeline completo de prepara√ß√£o de dados para Machine Learning**, em um problema de **sequ√™ncia para sequ√™ncia (seq2seq)** simples.\n",
        "Vamos analisar cada parte do resultado com foco em como isso se relaciona com o aprendizado de m√°quina üëá\n",
        "\n",
        "---\n",
        "\n",
        "## üß© 1Ô∏è‚É£ Vocabul√°rio e codifica√ß√£o\n",
        "\n",
        "```\n",
        "Vocabul√°rio: {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, ' ': 5}\n",
        "Tamanho do vocabul√°rio: 6\n",
        "```\n",
        "\n",
        "üëâ Isso mostra que voc√™ criou um **espa√ßo discreto de s√≠mbolos** ‚Äî uma esp√©cie de *mini universo lingu√≠stico*.\n",
        "Cada caractere foi convertido em um **√≠ndice num√©rico √∫nico**, e esse processo √© equivalente ao que acontece em NLP (Natural Language Processing), quando as palavras s√£o transformadas em IDs antes de virar embeddings.\n",
        "\n",
        "**Interpreta√ß√£o ML:**\n",
        "\n",
        "* Este √© o **pr√©-processamento** de um modelo de linguagem.\n",
        "* No lugar de palavras, aqui temos letras.\n",
        "* Esse mapeamento (`char ‚Üí int`) √© a base para permitir que redes neurais operem sobre **n√∫meros** em vez de texto.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 2Ô∏è‚É£ Dados de entrada e sa√≠da ‚Äî um problema *seq2seq*\n",
        "\n",
        "```\n",
        "Exemplo 1:\n",
        "  Original: acdba\n",
        "  Codificado: [0, 2, 3, 1, 0]\n",
        "  Decodificado: acdba\n",
        "  Reverso (target esperado): abdca\n",
        "```\n",
        "\n",
        "Aqui voc√™ definiu um **problema supervisionado** cl√°ssico:\n",
        "\n",
        "* Entrada: uma sequ√™ncia de letras (`acdba`)\n",
        "* Sa√≠da esperada (target): a **sequ√™ncia invertida** (`abdca`)\n",
        "\n",
        "‚úÖ Isso √© um *toy problem* (problema de brinquedo) que ajuda a testar se uma rede neural consegue **aprender padr√µes de sequ√™ncia**.\n",
        "Em vez de traduzir entre idiomas (como o Encoder‚ÄìDecoder faz em tradu√ß√£o), aqui ela precisa **aprender a inverter a sequ√™ncia** ‚Äî uma tarefa simples, mas perfeita para estudar aprendizado seq√ºencial.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ 3Ô∏è‚É£ Estruturas dos tensores\n",
        "\n",
        "```\n",
        "inputs:  torch.Size([50000, 5])\n",
        "targets: torch.Size([50000, 5])\n",
        "```\n",
        "\n",
        "Isso quer dizer:\n",
        "\n",
        "* Temos **50.000 exemplos de treino**.\n",
        "* Cada exemplo √© uma **sequ√™ncia de 5 tokens**.\n",
        "\n",
        "üìä Cada linha √© um exemplo (amostra) e cada coluna √© uma posi√ß√£o da sequ√™ncia (caractere).\n",
        "Portanto, o modelo ver√° isso como uma **matriz de tamanho 50.000 √ó 5** ‚Äî um *dataset tabular temporal*.\n",
        "\n",
        "**Do ponto de vista de ML:**\n",
        "\n",
        "* Cada linha = uma observa√ß√£o.\n",
        "* Cada coluna = uma dimens√£o temporal (ou ‚Äúposi√ß√£o‚Äù no texto).\n",
        "* Isso est√° no formato ideal para entrar em uma rede neural do tipo **RNN**, **LSTM** ou **Transformer Encoder‚ÄìDecoder**.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ 4Ô∏è‚É£ Batch de treinamento\n",
        "\n",
        "```\n",
        "Entradas (xb): torch.Size([128, 5])\n",
        "Alvos (yb): torch.Size([128, 5])\n",
        "```\n",
        "\n",
        "O **DataLoader** est√° dividindo o dataset em *batches* de 128 exemplos.\n",
        "Isso √© essencial para **treinamento eficiente** e **c√°lculo vetorizado em GPU**.\n",
        "\n",
        "**Por que isso √© importante:**\n",
        "\n",
        "* As redes neurais aprendem com *gradientes m√©dios por lote*, n√£o amostra a amostra.\n",
        "* Isso acelera o treino e suaviza o processo de otimiza√ß√£o (SGD, Adam, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üß© 5Ô∏è‚É£ Confer√™ncia de um batch real\n",
        "\n",
        "```\n",
        "Exemplo de entrada decodificada: caebd\n",
        "Exemplo de alvo decodificado: dbeac\n",
        "```\n",
        "\n",
        "Aqui voc√™ confirmou que:\n",
        "\n",
        "* A **entrada** √© uma sequ√™ncia aleat√≥ria.\n",
        "* O **alvo** √© essa sequ√™ncia invertida.\n",
        "  Isso mostra que o dataset est√° **coerente e limpo**, pronto para o modelo aprender o mapeamento.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 6Ô∏è‚É£ Interpreta√ß√£o conceitual (vis√£o de ML)\n",
        "\n",
        "| Etapa                | Conceito ML                        | Analogia                                   |\n",
        "| -------------------- | ---------------------------------- | ------------------------------------------ |\n",
        "| Codifica√ß√£o          | Transformar s√≠mbolos em n√∫meros    | Dicion√°rio de tokens                       |\n",
        "| Padding              | Normalizar tamanho das sequ√™ncias  | Preencher com ‚Äúespa√ßo‚Äù                     |\n",
        "| Dataset + Dataloader | Estrutura de treino supervisionado | Como ‚Äúperguntas e respostas‚Äù para o modelo |\n",
        "| Input/Target         | Aprendizado seq2seq                | Entrada ‚Üí Sa√≠da esperada                   |\n",
        "| Batch                | Otimiza√ß√£o por gradiente           | Treino em mini-grupos                      |\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Conclus√£o\n",
        "\n",
        "‚úÖ **O dataset est√° bem constru√≠do.**\n",
        "Voc√™ implementou, sem usar bibliotecas externas, o pipeline completo que qualquer sistema de NLP moderno (inclusive LLMs) usa em escala ‚Äî apenas de forma reduzida e did√°tica.\n",
        "\n",
        "üöÄ **Pr√≥ximo passo natural:**\n",
        "\n",
        "* Criar um modelo simples (por exemplo, `nn.Embedding + nn.LSTM + nn.Linear`)\n",
        "* Trein√°-lo para aprender a tarefa de revers√£o (seq2seq)\n",
        "* Observar se o *loss* diminui e se o modelo aprende a gerar a sequ√™ncia invertida.\n",
        "\n",
        "Se quiser, posso gerar esse modelo de rede neural (Encoder‚ÄìDecoder m√≠nimo em PyTorch) para continuar o experimento. Quer seguir para isso?\n"
      ],
      "metadata": {
        "id": "oh1J82hVwgxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Veja um par"
      ],
      "metadata": {
        "id": "P43qdDKt59V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs[1])"
      ],
      "metadata": {
        "id": "H9GyPfjahI_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(tensor([3, 0, 3, 3, 2]), tensor([2, 3, 3, 0, 3]))\n"
      ],
      "metadata": {
        "id": "fOXwxDQ6h4GG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defini√ß√£o do modelo Seq2Seq com GRU"
      ],
      "metadata": {
        "id": "A5ddkeXUhDBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        _, h = self.gru(x)\n",
        "        return h  # [1, B, H]\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        \"\"\"\n",
        "        x: tensor que indica a parte pr√©via correta\n",
        "        h: tensor que indica o estado do encoder da parte pr√©via\n",
        "        \"\"\"\n",
        "        x = self.embed(x)\n",
        "        out, h = self.gru(x, h)\n",
        "        logits = self.fc(out)\n",
        "        return logits, h # retorna o estado latente para atualizar o estado\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        h = self.encoder(src)\n",
        "        # usa contexto correto anterior e estado atual para prever o tgt[:, -1]\n",
        "        logits, _ = self.decoder(tgt[:, :-1], h)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "d-1LP2kIdh4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C√≥digo para usar o modelo treinado: infer√™ncia\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "o39huAV1soHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_step(decoder, token, h):\n",
        "    logits, h = decoder(token, h) # obt√©m logits e atualiza estado da sequ√™ncia\n",
        "    next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "    return next_token, h\n",
        "\n",
        "def predict(model, seq, max_len=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = pad(encode(seq)).unsqueeze(0).to(device, dtype=torch.long)\n",
        "        h = model.encoder(src) # Obt√©m estado do modelo ap√≥s processar entrada inicial\n",
        "\n",
        "        # 'token' representa a gera√ß√£o passo a passo da sequ√™ncia invertida\n",
        "        token = torch.tensor([[vocab[' ']]], dtype=torch.long, device=device)\n",
        "        seq_invertida = []\n",
        "        for _ in range(max_len):\n",
        "            token, h = decode_step(model.decoder, token, h)\n",
        "            seq_invertida.append(token.item())\n",
        "        return decode(seq_invertida)"
      ],
      "metadata": {
        "id": "x6MshjDHsnbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepara√ß√£o para treino"
      ],
      "metadata": {
        "id": "qVzYCLNjg_ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_size = 32\n",
        "hidden_size = 64\n",
        "encoder = Encoder(vocab_size, emb_size, hidden_size)\n",
        "decoder = Decoder(vocab_size, emb_size, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab[' ']) # ignora o pad: \" \"\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "-abFA-NmeJzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execu√ß√£o do treino"
      ],
      "metadata": {
        "id": "JacyZGK940n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device, dtype=torch.long), yb.to(device, dtype=torch.long)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb, yb)\n",
        "        loss = loss_fn(logits.reshape(-1, vocab_size), yb[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")"
      ],
      "metadata": {
        "id": "rKl4t82iePsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3150c8f2"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        _, h = self.gru(x)\n",
        "        return h  # [1, B, H]\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        \"\"\"\n",
        "        x: tensor que indica a parte pr√©via correta\n",
        "        h: tensor que indica o estado do encoder da parte pr√©via\n",
        "        \"\"\"\n",
        "        x = self.embed(x)\n",
        "        out, h = self.gru(x, h)\n",
        "        logits = self.fc(out)\n",
        "        return logits, h # retorna o estado latente para atualizar o estado\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        h = self.encoder(src)\n",
        "        # usa contexto correto anterior e estado atual para prever o tgt[:, -1]\n",
        "        logits, _ = self.decoder(tgt[:, :-1], h)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c24a5032"
      },
      "source": [
        "emb_size = 32\n",
        "hidden_size = 64\n",
        "encoder = Encoder(vocab_size, emb_size, hidden_size)\n",
        "decoder = Decoder(vocab_size, emb_size, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab[' ']) # ignora o pad: \" \"\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"Model initialized and moved to device: {device}\")\n",
        "print(f\"Model architecture:\\n{model}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vamos testar"
      ],
      "metadata": {
        "id": "-hnWdfoq4xL_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDDLTV7CeVa7"
      },
      "source": [
        "for _ in range(10):\n",
        "    s = random_seq()\n",
        "    print(f\"Input string: '{s}'\") # Added print to show the generated input\n",
        "    pred = predict(model, s, max_len=len(s))\n",
        "    print(f\"{s} -> {pred}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d953ef"
      },
      "source": [
        "# Task\n",
        "Evaluate the accuracy of the trained sequence inversion model by generating 100 new random sequences, predicting their inversions, calculating the percentage of correct predictions, and displaying examples of both correct and incorrect predictions, along with the overall accuracy and a conclusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5a4b085"
      },
      "source": [
        "## Gerar dados de teste\n",
        "\n",
        "### Subtask:\n",
        "Gerar um novo conjunto de 100 sequ√™ncias aleat√≥rias e suas invers√µes correspondentes para usar como dados de teste.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1015e0e"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to generate 100 random sequences and their reversed counterparts for testing, as instructed by the subtask. This will create a test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfa2218a"
      },
      "source": [
        "test_pairs = []\n",
        "for _ in range(100):\n",
        "    s = random_seq() # Gera uma sequ√™ncia aleat√≥ria\n",
        "    s_reversed = s[::-1] # Inverte a sequ√™ncia\n",
        "    test_pairs.append((s, s_reversed))\n",
        "\n",
        "print(f\"Gerado {len(test_pairs)} pares de teste.\")\n",
        "print(f\"Primeiros 3 exemplos de pares de teste: {test_pairs[:3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "416ad259"
      },
      "source": [
        "## Avaliar precis√£o do modelo\n",
        "\n",
        "### Subtask:\n",
        "Usar o modelo treinado para prever a invers√£o de cada sequ√™ncia no conjunto de teste e calcular a acur√°cia (porcentagem de previs√µes corretas).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a28c5a7"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to evaluate the model's performance by iterating through the test pairs, making predictions, comparing them with the expected outcomes, and then calculating and displaying the accuracy along with examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c786a40"
      },
      "source": [
        "correct_predictions_count = 0\n",
        "total_tests = 0\n",
        "correct_examples = []\n",
        "incorrect_examples = []\n",
        "\n",
        "for original_seq, expected_reversed_seq in test_pairs:\n",
        "    predicted_reversed_seq = predict(model, original_seq, max_len=len(original_seq))\n",
        "    total_tests += 1\n",
        "\n",
        "    if predicted_reversed_seq == expected_reversed_seq:\n",
        "        correct_predictions_count += 1\n",
        "        correct_examples.append((original_seq, predicted_reversed_seq, expected_reversed_seq))\n",
        "    else:\n",
        "        incorrect_examples.append((original_seq, predicted_reversed_seq, expected_reversed_seq))\n",
        "\n",
        "accuracy = (correct_predictions_count / total_tests) * 100\n",
        "\n",
        "print(f\"\\n--- Avalia√ß√£o da Acur√°cia ---\")\n",
        "print(f\"Total de testes: {total_tests}\")\n",
        "print(f\"Previs√µes corretas: {correct_predictions_count}\")\n",
        "print(f\"Previs√µes incorretas: {total_tests - correct_predictions_count}\")\n",
        "print(f\"Acur√°cia do modelo: {accuracy:.2f}%\")\n",
        "\n",
        "print(f\"\\n--- Exemplos de Previs√µes Corretas ({len(correct_examples)} amostras) ---\")\n",
        "for i, (original, predicted, expected) in enumerate(correct_examples[:5]): # Mostrar at√© 5 exemplos\n",
        "    print(f\"  Original: '{original}' -> Previsto: '{predicted}' (Esperado: '{expected}')\")\n",
        "\n",
        "print(f\"\\n--- Exemplos de Previs√µes Incorretas ({len(incorrect_examples)} amostras) ---\")\n",
        "for i, (original, predicted, expected) in enumerate(incorrect_examples[:5]): # Mostrar at√© 5 exemplos\n",
        "    print(f\"  Original: '{original}' -> Previsto: '{predicted}' (Esperado: '{expected}')\")\n",
        "\n",
        "print(f\"\\nConclus√£o: O modelo obteve uma acur√°cia de {accuracy:.2f}% na tarefa de invers√£o de sequ√™ncias.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ab3b64f"
      },
      "source": [
        "## Finalizar avalia√ß√£o\n",
        "\n",
        "### Subtask:\n",
        "Apresentar a acur√°cia geral do modelo e fazer uma conclus√£o sobre o desempenho, incluindo a revis√£o dos exemplos de erros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaf89c0e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The model's overall accuracy is 0.00%.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A test set of 100 random sequences and their inversions was generated.\n",
        "*   The model was evaluated on these 100 test sequences.\n",
        "*   The model achieved an accuracy of 0.00%, making 0 correct predictions out of 100 test cases.\n",
        "*   All 100 predictions made by the model were incorrect.\n",
        "*   Examples of incorrect predictions show that the model failed to invert sequences correctly (e.g., for 'cdbed', the model predicted 'bcbbb' while the expected inversion was 'debdc').\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The model's performance on the sequence inversion task is extremely poor, indicating a fundamental issue with its training or architecture.\n",
        "*   The next step should be to investigate the reasons for this failure, such as reviewing the training data, the model's architecture, and the training process, followed by retraining the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aba9d6e2"
      },
      "source": [
        "# Task\n",
        "The model's current accuracy is 0.00%, indicating a fundamental issue. The next step is to debug the model's logic, specifically by introducing 'Start Of Sequence' (SOS) and 'End Of Sequence' (EOS) tokens. This involves modifying the vocabulary, adjusting data preparation to include these tokens in target sequences, updating the `Seq2Seq` model's forward pass, and revising the `predict` function to use SOS for initiation and EOS for termination. After these changes, the model needs to be retrained, followed by an re-evaluation of its accuracy to determine if the corrections improved performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "291fea4e"
      },
      "source": [
        "## Depurar L√≥gica do Modelo e Treinamento\n",
        "\n",
        "### Subtask:\n",
        "Revisar o m√©todo `forward` do modelo `Seq2Seq`, prestando aten√ß√£o especial a como a entrada do decodificador (`tgt[:, :-1]`) e o alvo para o c√°lculo da perda (`yb[:, 1:]`) s√£o tratados. Verificar as formas e a indexa√ß√£o para a `loss_fn`. Al√©m disso, inspecionar a fun√ß√£o `predict`, particularmente o token inicial passado para o decodificador e o loop de gera√ß√£o.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1162c95a"
      },
      "source": [
        "**Reasoning**:\n",
        "The current model fails because the decoder is never trained to predict the first token of the target sequence. To fix this, I need to introduce a special Start-Of-Sequence (SOS) token into the vocabulary and data generation. This SOS token will be prepended to all target sequences during training, allowing the decoder to learn to predict the first actual character when given the SOS token as input. I also need to adjust the `max_len` for padding to accommodate the new token. Finally, the `predict` function must be updated to use this SOS token as the initial input to the decoder during inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36cab38f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# ===== 1. Dicion√°rio e fun√ß√µes b√°sicas (Atualizado) =====\n",
        "# Adiciona um token <SOS> (Start-Of-Sequence) ao vocabul√°rio\n",
        "chars = list(\"abcde\") # Os caracteres originais\n",
        "SOS_TOKEN = '<S>'\n",
        "PAD_TOKEN = ' '\n",
        "\n",
        "# Reordena para que o espa√ßo seja o √∫ltimo para o ignore_index e SOS um pouco antes\n",
        "chars.append(SOS_TOKEN)\n",
        "chars.append(PAD_TOKEN)\n",
        "\n",
        "vocab = {ch: i for i, ch in enumerate(chars)}  # Mapeia cada caractere para um √≠ndice num√©rico\n",
        "inv_vocab = {i: ch for ch, i in vocab.items()} # Cria um dicion√°rio inverso para decodificar √≠ndices para caracteres\n",
        "vocab_size = len(vocab)  # Quantidade total de tokens poss√≠veis\n",
        "\n",
        "def encode(s):  # Converte uma string em uma sequ√™ncia de √≠ndices num√©ricos\n",
        "    return torch.tensor([vocab[c] for c in s], dtype=torch.long)\n",
        "\n",
        "def decode(t):  # Converte uma sequ√™ncia de √≠ndices num√©ricos de volta para string\n",
        "    return ''.join(inv_vocab[int(x)] for x in t if inv_vocab[int(x)] != PAD_TOKEN and inv_vocab[int(x)] != SOS_TOKEN)\n",
        "\n",
        "def random_seq(n=5):  # Gera uma sequ√™ncia aleat√≥ria de tamanho n usando apenas 'abcde'\n",
        "    return ''.join(random.choice(chars[:-2]) for _ in range(n))  # Exclui SOS_TOKEN e PAD_TOKEN\n",
        "\n",
        "# ===== 2. Gerar dados (Atualizado) =====\n",
        "# Prepend <SOS> ao target invertido para que o decoder possa aprender a predizer o primeiro token\n",
        "# O 'max_len' ser√° o comprimento original + 1 (para o token SOS)\n",
        "\n",
        "# Geramos as sequ√™ncias originais e invertidas, com <SOS> prependido para o target\n",
        "raw_pairs = []\n",
        "for _ in range(50000):\n",
        "    s = random_seq()\n",
        "    s_reversed_with_sos = SOS_TOKEN + s[::-1] # Adiciona <SOS> ao in√≠cio da sequ√™ncia invertida\n",
        "    raw_pairs.append((s, s_reversed_with_sos))\n",
        "\n",
        "# Calcula o comprimento m√°ximo *ap√≥s* adicionar o token SOS ao target\n",
        "max_len_src = max(len(s) for s, _ in raw_pairs)\n",
        "max_len_tgt = max(len(s_rev) for _, s_rev in raw_pairs) # Ser√° max_len_src + 1\n",
        "\n",
        "def pad_src(x):  # Preenche a sequ√™ncia de origem com espa√ßos para padronizar o tamanho\n",
        "    return torch.cat([encode(x), torch.tensor([vocab[PAD_TOKEN]] * (max_len_src - len(x)))], dim=0)\n",
        "\n",
        "def pad_tgt(y):  # Preenche a sequ√™ncia alvo (com SOS) com espa√ßos\n",
        "    return torch.cat([encode(y), torch.tensor([vocab[PAD_TOKEN]] * (max_len_tgt - len(y)))], dim=0)\n",
        "\n",
        "inputs = torch.stack([pad_src(s) for s, _ in raw_pairs])\n",
        "targets = torch.stack([pad_tgt(s_rev) for _, s_rev in raw_pairs])\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(inputs, targets)\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ===== 3. Prints para inspecionar (Atualizado) =====\n",
        "print(f\"Vocabul√°rio: {vocab}\")\n",
        "print(f\"Tamanho do vocabul√°rio: {vocab_size}\")\n",
        "print(f\"Tamanho m√°ximo das sequ√™ncias de origem (max_len_src): {max_len_src}\")\n",
        "print(f\"Tamanho m√°ximo das sequ√™ncias alvo (max_len_tgt): {max_len_tgt}\")\n",
        "\n",
        "# Mostrar 3 exemplos codificados/decodificados\n",
        "for i in range(3):\n",
        "    s = random_seq()\n",
        "    encoded_src = encode(s)\n",
        "    encoded_tgt_with_sos = encode(SOS_TOKEN + s[::-1])\n",
        "    decoded_src = decode(encoded_src)\n",
        "    decoded_tgt_with_sos = decode(encoded_tgt_with_sos)\n",
        "    print(f\"\\nExemplo {i+1}:\")\n",
        "    print(f\"  Original (src): '{s}'\")\n",
        "    print(f\"  Codificado (src): {encoded_src.tolist()}\")\n",
        "    print(f\"  Decodificado (src): '{decoded_src}'\")\n",
        "    print(f\"  Reverso com SOS (target esperado): '{SOS_TOKEN}{s[::-1]}'\")\n",
        "    print(f\"  Codificado (target): {encoded_tgt_with_sos.tolist()}\")\n",
        "    print(f\"  Decodificado (target): '{decoded_tgt_with_sos}'\")\n",
        "\n",
        "# Mostrar formas (shapes) dos tensores de entrada e sa√≠da\n",
        "print(\"\\nShapes:\")\n",
        "print(f\"  inputs:  {inputs.shape}\")\n",
        "print(f\"  targets: {targets.shape}\")\n",
        "\n",
        "# Mostrar o primeiro batch do DataLoader\n",
        "for xb, yb in train_dl:\n",
        "    print(\"\\nPrimeiro batch de treino:\")\n",
        "    print(\"  Entradas (xb):\", xb.shape)\n",
        "    print(\"  Alvos (yb):\", yb.shape)\n",
        "    print(\"  Exemplo de entrada decodificada:\", decode(xb[0]))\n",
        "    print(\"  Exemplo de alvo decodificada:\", decode(yb[0]))\n",
        "    break\n",
        "\n",
        "# --- Atualiza√ß√£o da fun√ß√£o predict para usar o SOS_TOKEN ---\n",
        "\n",
        "def predict(model, seq, max_len_output): # max_len_output deve ser o len original da seq\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = pad_src(seq).unsqueeze(0).to(device, dtype=torch.long)\n",
        "        h = model.encoder(src)\n",
        "\n",
        "        # O token inicial para o decoder √© agora o SOS_TOKEN\n",
        "        token = torch.tensor([[vocab[SOS_TOKEN]]], dtype=torch.long, device=device)\n",
        "        seq_invertida = []\n",
        "\n",
        "        for _ in range(max_len_output): # Gera o n√∫mero de caracteres esperado (sem SOS)\n",
        "            token, h = decode_step(model.decoder, token, h)\n",
        "            # Ignora tokens de padding se por acaso o modelo os gerar antes do esperado\n",
        "            if inv_vocab[token.item()] == PAD_TOKEN:\n",
        "                break\n",
        "            seq_invertida.append(token.item())\n",
        "\n",
        "        return decode(seq_invertida)\n",
        "\n",
        "# Re-definir o modelo e otimizador com o novo vocab_size\n",
        "emb_size = 32\n",
        "hidden_size = 64\n",
        "encoder = Encoder(vocab_size, emb_size, hidden_size)\n",
        "decoder = Decoder(vocab_size, emb_size, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# loss_fn continua o mesmo, ignorando PAD_TOKEN (que √© o ' ')\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"Model initialized with updated vocab_size: {vocab_size} and moved to device: {device}\")\n",
        "print(f\"Model architecture:\\n{model}\")\n",
        "\n",
        "# Execu√ß√£o do treino (copiado do bloco anterior, mas usando as novas defini√ß√µes)\n",
        "print(\"\\nIniciando treinamento...\")\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device, dtype=torch.long), yb.to(device, dtype=torch.long)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb, yb) # yb √© agora [B, max_len_tgt] com SOS prepended\n",
        "\n",
        "        # loss_fn espera logits de [N, C] e target de [N]\n",
        "        # logits √© [B, max_len_tgt-1, vocab_size] => reshape para [B*(max_len_tgt-1), vocab_size]\n",
        "        # yb[:, 1:] √© o target real (sem SOS) => reshape para [B*(max_len_tgt-1)]\n",
        "        loss = loss_fn(logits.reshape(-1, vocab_size), yb[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b21a0f1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error 'KeyError: '<'' occurred because the `encode` function was trying to process the multi-character `SOS_TOKEN` ('<S>') as individual characters ('<' and 'S'), which are not present in the vocabulary. To fix this, I need to modify the data generation to create target sequences as lists of tokens (e.g., `['<S>', 'a', 'b']`) instead of a concatenated string (e.g., `'<S>ab'`). The `encode` function needs to be updated to handle these token lists correctly. Additionally, the print statements in the inspection section need to reflect this change.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ac300a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# ===== 1. Dicion√°rio e fun√ß√µes b√°sicas (Atualizado) =====\n",
        "# Adiciona um token <SOS> (Start-Of-Sequence) ao vocabul√°rio\n",
        "chars = list(\"abcde\") # Os caracteres originais\n",
        "SOS_TOKEN = '<S>'\n",
        "PAD_TOKEN = ' '\n",
        "\n",
        "# Reordena para que o espa√ßo seja o √∫ltimo para o ignore_index e SOS um pouco antes\n",
        "chars.append(SOS_TOKEN)\n",
        "chars.append(PAD_TOKEN)\n",
        "\n",
        "vocab = {ch: i for i, ch in enumerate(chars)}  # Mapeia cada caractere para um √≠ndice num√©rico\n",
        "inv_vocab = {i: ch for ch, i in vocab.items()} # Cria um dicion√°rio inverso para decodificar √≠ndices para caracteres\n",
        "vocab_size = len(vocab)  # Quantidade total de tokens poss√≠veis\n",
        "\n",
        "def encode(s):  # Converte uma string de caracteres ou uma lista de tokens em uma sequ√™ncia de √≠ndices num√©ricos\n",
        "    # This function is now designed to take either a string (for source sequences) or a list of tokens (for target sequences)\n",
        "    # If it's a string, it assumes it's composed of single characters in 'chars[:-2]'\n",
        "    # If it's a list, it assumes each element is a valid token (char or SOS_TOKEN)\n",
        "    if isinstance(s, list):\n",
        "        return torch.tensor([vocab[c] for c in s], dtype=torch.long)\n",
        "    else: # Assume string input, composed of single characters\n",
        "        return torch.tensor([vocab[c] for c in s], dtype=torch.long)\n",
        "\n",
        "def decode(t):  # Converte uma sequ√™ncia de √≠ndices num√©ricos de volta para string\n",
        "    return ''.join(inv_vocab[int(x)] for x in t if inv_vocab[int(x)] != PAD_TOKEN and inv_vocab[int(x)] != SOS_TOKEN)\n",
        "\n",
        "def random_seq(n=5):\n",
        "    return ''.join(random.choice(chars[:-2]) for _ in range(n))  # Exclui SOS_TOKEN e PAD_TOKEN\n",
        "\n",
        "# ===== 2. Gerar dados (Atualizado) =====\n",
        "# Prepend <SOS> ao target invertido para que o decoder possa aprender a predizer o primeiro token\n",
        "# O 'max_len' ser√° o comprimento original + 1 (para o token SOS)\n",
        "\n",
        "# Geramos as sequ√™ncias originais e invertidas, com <SOS> prependido para o target\n",
        "raw_pairs = []\n",
        "for _ in range(50000):\n",
        "    s = random_seq()\n",
        "    # Adiciona <SOS> como um token separado ao in√≠cio da lista de tokens invertidos\n",
        "    s_reversed_tokens = [SOS_TOKEN] + list(s[::-1])\n",
        "    raw_pairs.append((s, s_reversed_tokens))\n",
        "\n",
        "# Calcula o comprimento m√°ximo *ap√≥s* adicionar o token SOS ao target\n",
        "max_len_src = max(len(s) for s, _ in raw_pairs)\n",
        "max_len_tgt = max(len(s_rev_tokens) for _, s_rev_tokens in raw_pairs) # Ser√° max_len_src + 1\n",
        "\n",
        "def pad_src(x):  # Preenche a sequ√™ncia de origem (string) com espa√ßos para padronizar o tamanho\n",
        "    return torch.cat([encode(x), torch.tensor([vocab[PAD_TOKEN]] * (max_len_src - len(x)))], dim=0)\n",
        "\n",
        "def pad_tgt(y_tokens):  # Preenche a sequ√™ncia alvo (lista de tokens) com espa√ßos\n",
        "    return torch.cat([encode(y_tokens), torch.tensor([vocab[PAD_TOKEN]] * (max_len_tgt - len(y_tokens)))], dim=0)\n",
        "\n",
        "inputs = torch.stack([pad_src(s) for s, _ in raw_pairs])\n",
        "targets = torch.stack([pad_tgt(s_rev_tokens) for _, s_rev_tokens in raw_pairs])\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(inputs, targets)\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ===== 3. Prints para inspecionar (Atualizado) =====\n",
        "print(f\"Vocabul√°rio: {vocab}\")\n",
        "print(f\"Tamanho do vocabul√°rio: {vocab_size}\")\n",
        "print(f\"Tamanho m√°ximo das sequ√™ncias de origem (max_len_src): {max_len_src}\")\n",
        "print(f\"Tamanho m√°ximo das sequ√™ncias alvo (max_len_tgt): {max_len_tgt}\")\n",
        "\n",
        "# Mostrar 3 exemplos codificados/decodificados\n",
        "for i in range(3):\n",
        "    s = random_seq()\n",
        "    encoded_src = encode(s)\n",
        "    # A sequ√™ncia alvo com SOS agora √© uma lista de tokens para codifica√ß√£o\n",
        "    encoded_tgt_with_sos = encode([SOS_TOKEN] + list(s[::-1]))\n",
        "    decoded_src = decode(encoded_src)\n",
        "    decoded_tgt_with_sos = decode(encoded_tgt_with_sos)\n",
        "    print(f\"\\nExemplo {i+1}:\")\n",
        "    print(f\"  Original (src): '{s}'\")\n",
        "    print(f\"  Codificado (src): {encoded_src.tolist()}\")\n",
        "    print(f\"  Decodificado (src): '{decoded_src}'\")\n",
        "    print(f\"  Reverso com SOS (target esperado): '{SOS_TOKEN}{s[::-1]}'\")\n",
        "    print(f\"  Codificado (target): {encoded_tgt_with_sos.tolist()}\")\n",
        "    print(f\"  Decodificado (target): '{decoded_tgt_with_sos}'\")\n",
        "\n",
        "# Mostrar formas (shapes) dos tensores de entrada e sa√≠da\n",
        "print(\"\\nShapes:\")\n",
        "print(f\"  inputs:  {inputs.shape}\")\n",
        "print(f\"  targets: {targets.shape}\")\n",
        "\n",
        "# Mostrar o primeiro batch do DataLoader\n",
        "for xb, yb in train_dl:\n",
        "    print(\"\\nPrimeiro batch de treino:\")\n",
        "    print(\"  Entradas (xb):\", xb.shape)\n",
        "    print(\"  Alvos (yb):\", yb.shape)\n",
        "    print(\"  Exemplo de entrada decodificada:\", decode(xb[0]))\n",
        "    print(\"  Exemplo de alvo decodificada:\", decode(yb[0]))\n",
        "    break\n",
        "\n",
        "# --- Atualiza√ß√£o da fun√ß√£o predict para usar o SOS_TOKEN ---\n",
        "\n",
        "def predict(model, seq, max_len_output): # max_len_output deve ser o len original da seq\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = pad_src(seq).unsqueeze(0).to(device, dtype=torch.long)\n",
        "        h = model.encoder(src)\n",
        "\n",
        "        # O token inicial para o decoder √© agora o SOS_TOKEN\n",
        "        token = torch.tensor([[vocab[SOS_TOKEN]]], dtype=torch.long, device=device)\n",
        "        seq_invertida = []\n",
        "\n",
        "        for _ in range(max_len_output): # Gera o n√∫mero de caracteres esperado (sem SOS)\n",
        "            token, h = decode_step(model.decoder, token, h)\n",
        "            # Ignora tokens de padding se por acaso o modelo os gerar antes do esperado\n",
        "            if inv_vocab[token.item()] == PAD_TOKEN:\n",
        "                break\n",
        "            seq_invertida.append(token.item())\n",
        "\n",
        "        return decode(seq_invertida)\n",
        "\n",
        "# Re-definir o modelo e otimizador com o novo vocab_size\n",
        "emb_size = 32\n",
        "hidden_size = 64\n",
        "encoder = Encoder(vocab_size, emb_size, hidden_size)\n",
        "decoder = Decoder(vocab_size, emb_size, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# loss_fn continua o mesmo, ignorando PAD_TOKEN (que √© o ' ')\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"Model initialized with updated vocab_size: {vocab_size} and moved to device: {device}\")\n",
        "print(f\"Model architecture:\\n{model}\")\n",
        "\n",
        "# Execu√ß√£o do treino (copiado do bloco anterior, mas usando as novas defini√ß√µes)\n",
        "print(\"\\nIniciando treinamento...\")\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device, dtype=torch.long), yb.to(device, dtype=torch.long)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb, yb) # yb √© agora [B, max_len_tgt] com SOS prepended\n",
        "\n",
        "        # loss_fn espera logits de [N, C] e target de [N]\n",
        "        # logits √© [B, max_len_tgt-1, vocab_size] => reshape para [B*(max_len_tgt-1), vocab_size]\n",
        "        # yb[:, 1:] √© o target real (sem SOS) => reshape para [B*(max_len_tgt-1)]\n",
        "        loss = loss_fn(logits.reshape(-1, vocab_size), yb[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(train_dl):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9652a9cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully re-trained the model with the updated data preparation and SOS token handling. Now, I need to re-evaluate the model's accuracy on a test set to confirm if the debugging efforts improved its performance. This involves generating a new test set, making predictions with the updated `predict` function, and calculating the accuracy as requested in the overall task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c02fc76"
      },
      "source": [
        "test_pairs = []\n",
        "for _ in range(100):\n",
        "    s = random_seq() # Gera uma sequ√™ncia aleat√≥ria\n",
        "    s_reversed = s[::-1] # Inverte a sequ√™ncia\n",
        "    test_pairs.append((s, s_reversed))\n",
        "\n",
        "print(f\"Gerado {len(test_pairs)} pares de teste.\")\n",
        "print(f\"Primeiros 3 exemplos de pares de teste: {test_pairs[:3]}\")\n",
        "\n",
        "correct_predictions_count = 0\n",
        "total_tests = 0\n",
        "correct_examples = []\n",
        "incorrect_examples = []\n",
        "\n",
        "for original_seq, expected_reversed_seq in test_pairs:\n",
        "    # Use the updated predict function\n",
        "    predicted_reversed_seq = predict(model, original_seq, max_len_output=len(original_seq))\n",
        "    total_tests += 1\n",
        "\n",
        "    if predicted_reversed_seq == expected_reversed_seq:\n",
        "        correct_predictions_count += 1\n",
        "        correct_examples.append((original_seq, predicted_reversed_seq, expected_reversed_seq))\n",
        "    else:\n",
        "        incorrect_examples.append((original_seq, predicted_reversed_seq, expected_reversed_seq))\n",
        "\n",
        "accuracy = (correct_predictions_count / total_tests) * 100\n",
        "\n",
        "print(f\"\\n--- Avalia√ß√£o da Acur√°cia ---\")\n",
        "print(f\"Total de testes: {total_tests}\")\n",
        "print(f\"Previs√µes corretas: {correct_predictions_count}\")\n",
        "print(f\"Previs√µes incorretas: {total_tests - correct_predictions_count}\")\n",
        "print(f\"Acur√°cia do modelo: {accuracy:.2f}%\")\n",
        "\n",
        "print(f\"\\n--- Exemplos de Previs√µes Corretas ({len(correct_examples)} amostras) ---\")\n",
        "for i, (original, predicted, expected) in enumerate(correct_examples[:5]): # Mostrar at√© 5 exemplos\n",
        "    print(f\"  Original: '{original}' -> Previsto: '{predicted}' (Esperado: '{expected}')\")\n",
        "\n",
        "print(f\"\\n--- Exemplos de Previs√µes Incorretas ({len(incorrect_examples)} amostras) ---\")\n",
        "for i, (original, predicted, expected) in enumerate(incorrect_examples[:5]): # Mostrar at√© 5 exemplos\n",
        "    print(f\"  Original: '{original}' -> Previsto: '{predicted}' (Esperado: '{expected}')\")\n",
        "\n",
        "print(f\"\\nConclus√£o: O modelo obteve uma acur√°cia de {accuracy:.2f}% na tarefa de invers√£o de sequ√™ncias.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7021c50"
      },
      "source": [
        "## Finalizar Tarefa\n",
        "\n",
        "### Subtask:\n",
        "Resumir a nova acur√°cia, analisar as melhorias e discutir os pr√≥ximos passos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bd90964"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "1.  **What is the new accuracy of the model after the changes?**\n",
        "    The model achieved an accuracy of 100.00% on the sequence inversion task.\n",
        "\n",
        "2.  **How did the corrections improve performance?**\n",
        "    The corrections, specifically the proper handling of the 'Start Of Sequence' (SOS) token in the vocabulary, data preparation, and `predict` function, resolved a `KeyError` and allowed the model to learn the task. This led to a significant decrease in training loss (from approximately 0.4125 to 0.0010 or 0.0000) and ultimately, perfect accuracy.\n",
        "\n",
        "3.  **What are the next steps?**\n",
        "    Future work could involve testing the model's robustness with more complex sequence patterns, longer sequences, or different types of sequence-to-sequence tasks to evaluate its generalization capabilities.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The initial model had an accuracy of 0.00%, indicating a fundamental flaw in its logic or implementation.\n",
        "*   An initial attempt to integrate `SOS_TOKEN` led to a `KeyError` because the `encode` function incorrectly processed the multi-character `SOS_TOKEN` as individual characters.\n",
        "*   After modifying the `encode` function to correctly handle `SOS_TOKEN` as a single unit and updating data generation to prepend `SOS_TOKEN` distinctly, the model trained successfully.\n",
        "*   Training loss decreased from an initial average of approximately 0.4125 to 0.0010 (with some epochs reaching 0.0000) over 20 epochs.\n",
        "*   Following the corrections and retraining, the model achieved a perfect accuracy of 100.00% on a test set of 100 sequence inversion examples.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The successful implementation and correct handling of special tokens like SOS are critical for sequence generation models to learn and perform effectively.\n",
        "*   Given the 100.00% accuracy on this specific task, the current model architecture and training approach are highly effective for simple sequence inversion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9218420a"
      },
      "source": [
        "# Task\n",
        "Modifique o loop de treinamento para incluir a impress√£o de exemplos de depura√ß√£o a cada 5 √©pocas, retreine o modelo com essa modifica√ß√£o e summarize os resultados observados e os pr√≥ximos passos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e2d2a00"
      },
      "source": [
        "## Modificar Loop de Treinamento com Sa√≠da de Depura√ß√£o\n",
        "\n",
        "### Subtask:\n",
        "Atualizar o loop de treinamento para incluir a impress√£o de exemplos de depura√ß√£o a cada 5 √©pocas, mostrando a entrada original, a previs√£o do modelo e o alvo esperado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831b36ea"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to update the existing training loop by adding debug print statements every 5 epochs. This requires modifying the `for epoch in range(20):` loop to include the generation of a random sequence, its prediction by the model, and then printing the original, predicted, and expected reversed sequences. I will include the full code block containing the model definition, initialization, and the modified training loop to ensure all dependencies are met, as per the guidelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea5af57d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# ===== 1. Dicion√°rio e fun√ß√µes b√°sicas (Atualizado) =====\n",
        "# Adiciona um token <SOS> (Start-Of-Sequence) ao vocabul√°rio\n",
        "chars = list(\"abcde\") # Os caracteres originais\n",
        "SOS_TOKEN = '<S>'\n",
        "PAD_TOKEN = ' '\n",
        "\n",
        "# Reordena para que o espa√ßo seja o √∫ltimo para o ignore_index e SOS um pouco antes\n",
        "chars.append(SOS_TOKEN)\n",
        "chars.append(PAD_TOKEN)\n",
        "\n",
        "vocab = {ch: i for i, ch in enumerate(chars)}  # Mapeia cada caractere para um √≠ndice num√©rico\n",
        "inv_vocab = {i: ch for ch, i in vocab.items()} # Cria um dicion√°rio inverso para decodificar √≠ndices para caracteres\n",
        "vocab_size = len(vocab)  # Quantidade total de tokens poss√≠veis\n",
        "\n",
        "def encode(s):  # Converte uma string de caracteres ou uma lista de tokens em uma sequ√™ncia de √≠ndices num√©ricos\n",
        "    if isinstance(s, list):\n",
        "        return torch.tensor([vocab[c] for c in s], dtype=torch.long)\n",
        "    else: # Assume string input, composed of single characters\n",
        "        return torch.tensor([vocab[c] for c in s], dtype=torch.long)\n",
        "\n",
        "def decode(t):  # Converte uma sequ√™ncia de √≠ndices num√©ricos de volta para string\n",
        "    return ''.join(inv_vocab[int(x)] for x in t if inv_vocab[int(x)] != PAD_TOKEN and inv_vocab[int(x)] != SOS_TOKEN)\n",
        "\n",
        "def random_seq(n=5):\n",
        "    return ''.join(random.choice(chars[:-2]) for _ in range(n))  # Exclui SOS_TOKEN e PAD_TOKEN\n",
        "\n",
        "# ===== 2. Gerar dados (Atualizado) =====\n",
        "# Prepend <SOS> ao target invertido para que o decoder possa aprender a predizer o primeiro token\n",
        "# O 'max_len' ser√° o comprimento original + 1 (para o token SOS)\n",
        "\n",
        "# Geramos as sequ√™ncias originais e invertidas, com <SOS> prependido para o target\n",
        "raw_pairs = []\n",
        "for _ in range(50000):\n",
        "    s = random_seq()\n",
        "    # Adiciona <SOS> como um token separado ao in√≠cio da lista de tokens invertidos\n",
        "    s_reversed_tokens = [SOS_TOKEN] + list(s[::-1])\n",
        "    raw_pairs.append((s, s_reversed_tokens))\n",
        "\n",
        "# Calcula o comprimento m√°ximo *ap√≥s* adicionar o token SOS ao target\n",
        "max_len_src = max(len(s) for s, _ in raw_pairs)\n",
        "max_len_tgt = max(len(s_rev_tokens) for _, s_rev_tokens in raw_pairs) # Ser√° max_len_src + 1\n",
        "\n",
        "def pad_src(x):  # Preenche a sequ√™ncia de origem (string) com espa√ßos para padronizar o tamanho\n",
        "    return torch.cat([encode(x), torch.tensor([vocab[PAD_TOKEN]] * (max_len_src - len(x)))], dim=0)\n",
        "\n",
        "def pad_tgt(y_tokens):  # Preenche a sequ√™ncia alvo (lista de tokens) com espa√ßos\n",
        "    return torch.cat([encode(y_tokens), torch.tensor([vocab[PAD_TOKEN]] * (max_len_tgt - len(y_tokens)))], dim=0)\n",
        "\n",
        "inputs = torch.stack([pad_src(s) for s, _ in raw_pairs])\n",
        "targets = torch.stack([pad_tgt(s_rev_tokens) for _, s_rev_tokens in raw_pairs])\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(inputs, targets)\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ===== 3. Prints para inspecionar (Atualizado) =====\n",
        "print(f\"Vocabul√°rio: {vocab}\")\n",
        "print(f\"Tamanho do vocabul√°rio: {vocab_size}\")\n",
        "print(f\"Tamanho m√°ximo das sequ√™ncias de origem (max_len_src): {max_len_src}\")\n",
        "print(f\"Tamanho m√°ximo das sequ√™ncias alvo (max_len_tgt): {max_len_tgt}\")\n",
        "\n",
        "# Mostrar 3 exemplos codificados/decodificados\n",
        "for i in range(3):\n",
        "    s = random_seq()\n",
        "    encoded_src = encode(s)\n",
        "    # A sequ√™ncia alvo com SOS agora √© uma lista de tokens para codifica√ß√£o\n",
        "    encoded_tgt_with_sos = encode([SOS_TOKEN] + list(s[::-1]))\n",
        "    decoded_src = decode(encoded_src)\n",
        "    decoded_tgt_with_sos = decode(encoded_tgt_with_sos)\n",
        "    print(f\"\\nExemplo {i+1}:\")\n",
        "    print(f\"  Original (src): '{s}'\")\n",
        "    print(f\"  Codificado (src): {encoded_src.tolist()}\")\n",
        "    print(f\"  Decodificado (src): '{decoded_src}'\")\n",
        "    print(f\"  Reverso com SOS (target esperado): '{SOS_TOKEN}{s[::-1]}'\")\n",
        "    print(f\"  Codificado (target): {encoded_tgt_with_sos.tolist()}\")\n",
        "    print(f\"  Decodificado (target): '{decoded_tgt_with_sos}'\")\n",
        "\n",
        "# Mostrar formas (shapes) dos tensores de entrada e sa√≠da\n",
        "print(\"\\nShapes:\")\n",
        "print(f\"  inputs:  {inputs.shape}\")\n",
        "print(f\"  targets: {targets.shape}\")\n",
        "\n",
        "# Mostrar o primeiro batch do DataLoader\n",
        "for xb, yb in train_dl:\n",
        "    print(\"\\nPrimeiro batch de treino:\")\n",
        "    print(\"  Entradas (xb):\", xb.shape)\n",
        "    print(\"  Alvos (yb):\", yb.shape)\n",
        "    print(\"  Exemplo de entrada decodificada:\", decode(xb[0]))\n",
        "    print(\"  Exemplo de alvo decodificada:\", decode(yb[0]))\n",
        "    break\n",
        "\n",
        "# --- Modelo Seq2Seq (Redefini√ß√£o) ---\n",
        "# As classes Encoder, Decoder e Seq2Seq s√£o definidas aqui novamente\n",
        "# para garantir que o contexto completo do modelo esteja dispon√≠vel.\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        _, h = self.gru(x)\n",
        "        return h  # [1, B, H]\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        \"\"\"\n",
        "        x: tensor que indica a parte pr√©via correta\n",
        "        h: tensor que indica o estado do encoder da parte pr√©via\n",
        "        \"\"\"\n",
        "        x = self.embed(x)\n",
        "        out, h = self.gru(x, h)\n",
        "        logits = self.fc(out)\n",
        "        return logits, h # retorna o estado latente para atualizar o estado\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        h = self.encoder(src)\n",
        "        # usa contexto correto anterior e estado atual para prever o tgt[:, -1]\n",
        "        logits, _ = self.decoder(tgt[:, :-1], h)\n",
        "        return logits\n",
        "\n",
        "# --- decode_step e predict (Redefini√ß√£o) ---\n",
        "def decode_step(decoder, token, h):\n",
        "    logits, h = decoder(token, h) # obt√©m logits e atualiza estado da sequ√™ncia\n",
        "    next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "    return next_token, h\n",
        "\n",
        "def predict(model, seq, max_len_output): # max_len_output deve ser o len original da seq\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = pad_src(seq).unsqueeze(0).to(device, dtype=torch.long)\n",
        "        h = model.encoder(src)\n",
        "\n",
        "        # O token inicial para o decoder √© agora o SOS_TOKEN\n",
        "        token = torch.tensor([[vocab[SOS_TOKEN]]], dtype=torch.long, device=device)\n",
        "        seq_invertida = []\n",
        "\n",
        "        for _ in range(max_len_output): # Gera o n√∫mero de caracteres esperado (sem SOS)\n",
        "            token, h = decode_step(model.decoder, token, h)\n",
        "            # Ignora tokens de padding se por acaso o modelo os gerar antes do esperado\n",
        "            if inv_vocab[token.item()] == PAD_TOKEN:\n",
        "                break\n",
        "            seq_invertida.append(token.item())\n",
        "\n",
        "        return decode(seq_invertida)\n",
        "\n",
        "# Re-definir o modelo e otimizador com o novo vocab_size\n",
        "emb_size = 32\n",
        "hidden_size = 64\n",
        "encoder = Encoder(vocab_size, emb_size, hidden_size)\n",
        "decoder = Decoder(vocab_size, emb_size, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# loss_fn continua o mesmo, ignorando PAD_TOKEN (que √© o ' ')\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"Model initialized with updated vocab_size: {vocab_size} and moved to device: {device}\")\n",
        "print(f\"Model architecture:\\n{model}\")\n",
        "\n",
        "# Execu√ß√£o do treino (com depura√ß√£o)\n",
        "print(\"\\nIniciando treinamento...\")\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_dl:\n",
        "        xb, yb = xb.to(device, dtype=torch.long), yb.to(device, dtype=torch.long)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb, yb) # yb √© agora [B, max_len_tgt] com SOS prepended\n",
        "\n",
        "        # loss_fn espera logits de [N, C] e target de [N]\n",
        "        # logits √© [B, max_len_tgt-1, vocab_size] => reshape para [B*(max_len_tgt-1), vocab_size]\n",
        "        # yb[:, 1:] √© o target real (sem SOS) => reshape para [B*(max_len_tgt-1)]\n",
        "        loss = loss_fn(logits.reshape(-1, vocab_size), yb[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss/len(train_dl)\n",
        "    print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}\")\n",
        "\n",
        "    # Condi√ß√£o para imprimir exemplos de depura√ß√£o a cada 5 √©pocas\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"--- Depura√ß√£o na Epoch {epoch+1} ---\")\n",
        "        # Gera uma nova sequ√™ncia aleat√≥ria\n",
        "        s_debug = random_seq()\n",
        "        seq_len_debug = len(s_debug)\n",
        "        # Sequ√™ncia invertida esperada\n",
        "        expected_reversed_seq_debug = s_debug[::-1]\n",
        "        # Previs√£o do modelo\n",
        "        predicted_reversed_seq_debug = predict(model, s_debug, max_len_output=seq_len_debug)\n",
        "\n",
        "        print(f\"  Original: '{s_debug}'\")\n",
        "        print(f\"  Previsto: '{predicted_reversed_seq_debug}'\")\n",
        "        print(f\"  Esperado: '{expected_reversed_seq_debug}'\")\n",
        "        print(\"-------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d27a8759"
      },
      "source": [
        "## Finalizar Tarefa\n",
        "\n",
        "### Subtask:\n",
        "Resumir a nova acur√°cia, analisar as melhorias e discutir os pr√≥ximos passos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a5dbef"
      },
      "source": [
        "## Finalizar Tarefa\n",
        "\n",
        "### Subtask:\n",
        "Resumir a nova acur√°cia, analisar as melhorias e discutir os pr√≥ximos passos.\n",
        "\n",
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "1.  **How did the debug outputs confirm model learning?**\n",
        "    The debug outputs printed every 5 epochs showed that the model's predictions (`Previsto`) quickly aligned with the `Esperado` (expected) reversed sequences, even for randomly generated input strings. By epoch 5, the model was already making perfect predictions for the displayed examples, indicating successful learning.\n",
        "\n",
        "2.  **What was the impact of the changes made (SOS token and debug prints)?**\n",
        "    The introduction of the SOS token and the corrected data handling (from the previous subtask) led to the model successfully learning the sequence inversion task, achieving 100% accuracy on test examples. The debug prints confirmed this learning process visually throughout the training, showing perfect predictions for sampled sequences early in training.\n",
        "\n",
        "3.  **What are the next steps?**\n",
        "    Given the model achieved 100% accuracy on this specific task, the next steps could involve exploring more complex sequence-to-sequence problems, such as:\n",
        "    *   Inverting longer sequences.\n",
        "    *   Handling a larger and more varied vocabulary.\n",
        "    *   Implementing an EOS (End-Of-Sequence) token for variable-length output generation.\n",
        "    *   Exploring more advanced architectures (e.g., Transformers).\n",
        "    *   Applying the principles to real-world NLP tasks like machine translation or text summarization.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The training loss decreased consistently and rapidly, reaching near zero values by the end of the 20 epochs.\n",
        "*   The debug examples shown during training consistently matched the expected reversed sequences, demonstrating that the model effectively learned the underlying mapping.\n",
        "*   This confirms that the previous architectural and data preparation fixes (especially regarding the SOS token) were critical and successful.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The visual debugging during training is a powerful tool to understand model behavior and quickly identify if the model is learning as expected.\n",
        "*   The current model is highly effective for simple, fixed-length sequence inversion. Its simplicity and effectiveness on this toy problem make it a good foundation for understanding more complex seq2seq models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1a34f8d"
      },
      "source": [
        "## Finalizar Tarefa\n",
        "\n",
        "### Subtask:\n",
        "Resumir a nova acur√°cia, analisar as melhorias e discutir os pr√≥ximos passos.\n",
        "\n",
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "1.  **How did the debug outputs confirm model learning?**\n",
        "    The debug outputs printed every 5 epochs showed that the model's predictions (`Previsto`) quickly aligned with the `Esperado` (expected) reversed sequences, even for randomly generated input strings. By epoch 5, the model was already making perfect predictions for the displayed examples, indicating successful learning.\n",
        "\n",
        "2.  **What was the impact of the changes made (SOS token and debug prints)?**\n",
        "    The introduction of the SOS token and the corrected data handling (from the previous subtask) led to the model successfully learning the sequence inversion task, achieving 100% accuracy on test examples. The debug prints confirmed this learning process visually throughout the training, showing perfect predictions for sampled sequences early in training.\n",
        "\n",
        "3.  **What are the next steps?**\n",
        "    Given the model achieved 100% accuracy on this specific task, the next steps could involve exploring more complex sequence-to-sequence problems, such as:\n",
        "    *   Inverting longer sequences.\n",
        "    *   Handling a larger and more varied vocabulary.\n",
        "    *   Implementing an EOS (End-Of-Sequence) token for variable-length output generation.\n",
        "    *   Exploring more advanced architectures (e.g., Transformers).\n",
        "    *   Applying the principles to real-world NLP tasks like machine translation or text summarization.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The training loss decreased consistently and rapidly, reaching near zero values by the end of the 20 epochs.\n",
        "*   The debug examples shown during training consistently matched the expected reversed sequences, demonstrating that the model effectively learned the underlying mapping.\n",
        "*   This confirms that the previous architectural and data preparation fixes (especially regarding the SOS token) were critical and successful.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The visual debugging during training is a powerful tool to understand model behavior and quickly identify if the model is learning as expected.\n",
        "*   The current model is highly effective for simple, fixed-length sequence inversion. Its simplicity and effectiveness on this toy problem make it a good foundation for understanding more complex seq2seq models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "369c6eab"
      },
      "source": [
        "## Finalizar Tarefa\n",
        "\n",
        "### Subtask:\n",
        "Resumir os resultados observados durante o treinamento com depura√ß√£o e discutir os pr√≥ximos passos, se houver.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ce8342a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "1.  **How did the debug outputs confirm model learning?**\n",
        "    The debug outputs, printed every 5 epochs, showed that the model's predictions (`Previsto`) quickly and accurately aligned with the `Esperado` (expected) reversed sequences for randomly generated input strings. By epoch 5, the model was already making perfect predictions for the displayed examples, indicating successful and rapid learning.\n",
        "\n",
        "2.  **What was the impact of the changes made (SOS token and debug prints)?**\n",
        "    The `SOS_TOKEN` (introduced in a previous step) was crucial for enabling the model to successfully learn the sequence inversion task. The debug prints visually confirmed this learning process by demonstrating perfect predictions for sampled sequences as early as epoch 5.\n",
        "\n",
        "3.  **What are the next steps?**\n",
        "    Given the model achieved 100% accuracy on this specific task, future work could involve exploring more complex sequence-to-sequence problems, such as:\n",
        "    *   Inverting longer sequences.\n",
        "    *   Handling a larger and more varied vocabulary.\n",
        "    *   Implementing an EOS (End-Of-Sequence) token for variable-length output generation.\n",
        "    *   Exploring more advanced architectures (e.g., Transformers).\n",
        "    *   Applying these principles to real-world NLP tasks like machine translation or text summarization.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The training loss consistently and rapidly decreased, reaching near-zero values by the end of the 20 epochs.\n",
        "*   The debug examples shown during training (every 5 epochs) consistently matched the expected reversed sequences, demonstrating that the model effectively learned the underlying mapping and achieved perfect predictions for sampled inputs by epoch 5.\n",
        "*   These results confirm that the previously implemented architectural and data preparation fixes, especially the inclusion of the Start-Of-Sequence (`SOS_TOKEN`), were critical and successful for the model to learn the task.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Visual debugging during training, by printing examples of predictions, is a powerful tool to understand model behavior and quickly verify if the model is learning as expected.\n",
        "*   The current model is highly effective for simple, fixed-length sequence inversion. Its success on this toy problem provides a strong foundation for understanding and tackling more complex sequence-to-sequence problems.\n"
      ]
    }
  ]
}