{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdinandrafols/IA_LLMs/blob/main/gsi073_aula0_support_vector_machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSI073 - T√≥picos Especiais de Intelig√™ncia Artificial\n",
        "\n",
        "Neste notebook, um tipo de Support Vector Machine Linear.\n",
        "\n",
        "\n",
        "## Prepara√ß√£o dos dados"
      ],
      "metadata": {
        "id": "Mwsc0ViVertv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmZxMYLGefOh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "\n",
        "# Preparar o dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data; y = iris.target\n",
        "\n",
        "X = X[y != 1] ; y = y[y != 1] # versicolor\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "y[y == 0] = -1  # SVM espera r√≥tulos em {-1, +1}\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32) # Tensor √© um tipo especial que suporta muitas dimens√µes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A nossa Support Vector Machine √© basicamente um hiperplano definido por *w* e *b* que melhor separa as classes."
      ],
      "metadata": {
        "id": "nUv-LKlIe9Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boa! üëè Voc√™ acabou de montar a parte mais chatinha da prepara√ß√£o de dados para fazer uma **SVM ‚Äúestilo SVM‚Äù** em PyTorch: binarizou o problema do Iris e colocou os r√≥tulos em **{-1, +1}**, que √© exatamente o que a formula√ß√£o cl√°ssica de SVM usa.\n",
        "\n",
        "Vamos analisar o que o seu c√≥digo fez e j√° te mostro o pr√≥ximo passo (definir o modelo e a loss tipo hinge).\n",
        "\n",
        "---\n",
        "\n",
        "## üß† O que o seu c√≥digo fez\n",
        "\n",
        "```python\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "```\n",
        "\n",
        "* Carregou o Iris (150 amostras, 3 classes: 0=setosa, 1=versicolor, 2=virginica)\n",
        "\n",
        "```python\n",
        "X = X[y != 1]\n",
        "y = y[y != 1]\n",
        "```\n",
        "\n",
        "* Aqui voc√™ **removeu a classe 1 (versicolor)**.\n",
        "* Ent√£o sobraram **duas classes**:\n",
        "\n",
        "  * `0` ‚Üí setosa\n",
        "  * `2` ‚Üí virginica\n",
        "* Isso transformou o problema em **bin√°rio**, o que √© √≥timo para testar uma SVM ‚Äúna m√£o‚Äù.\n",
        "\n",
        "```python\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "y[y == 0] = -1\n",
        "```\n",
        "\n",
        "* Convertendo os r√≥tulos para tensor‚Ä¶\n",
        "* ‚Ä¶e trocando a classe `0` por `-1`.\n",
        "* Resultado final:\n",
        "\n",
        "  * setosa ‚Üí **-1**\n",
        "  * virginica ‚Üí **2** ‚ùó\n",
        "\n",
        "‚ö†Ô∏è Aqui tem um detalhe importante:\n",
        "voc√™ **n√£o converteu a classe 2 para +1**. Ent√£o seus r√≥tulos ficaram assim:\n",
        "\n",
        "* `-1` para a classe 0\n",
        "* `2` para a classe 2\n",
        "\n",
        "Mas para SVM o ideal √© **{-1, +1}**. Ent√£o precisamos fazer mais uma linha para mapear o 2 ‚Üí +1.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Vers√£o corrigida da prepara√ß√£o\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "\n",
        "# 1. Carregar dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Ficar s√≥ com 2 classes (excluir a classe 1 = versicolor)\n",
        "mask = (y != 1)\n",
        "X = X[mask]\n",
        "y = y[mask]   # agora y tem s√≥ 0 e 2\n",
        "\n",
        "# 3. Converter para {-1, +1}\n",
        "#  - classe 0 (setosa)  -> -1\n",
        "#  - classe 2 (virginica) -> +1\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "y[y == 0] = -1.0\n",
        "y[y == 2] = 1.0\n",
        "\n",
        "# 4. Converter X para tensor\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "print(\"Shape X:\", X.shape)\n",
        "print(\"Valores √∫nicos de y:\", y.unique())\n",
        "```\n",
        "\n",
        "Se voc√™ rodar isso, deve ver algo assim:\n",
        "\n",
        "```text\n",
        "Shape X: torch.Size([100, 4])\n",
        "Valores √∫nicos de y: tensor([-1.,  1.])\n",
        "```\n",
        "\n",
        "üëÜ Isso significa:\n",
        "\n",
        "* Ficamos com 100 amostras (50 setosa, 50 virginica)\n",
        "* Agora o problema est√° no formato perfeito para uma SVM.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Pr√≥ximo passo\n",
        "\n",
        "Agora que os dados est√£o prontos, d√° pra fazer uma SVM ‚Äúna unha‚Äù em PyTorch de 2 jeitos:\n",
        "\n",
        "1. **SVM primal simplificada**\n",
        "\n",
        "   * modelo linear: `f(x) = w¬∑x + b`\n",
        "   * loss = hinge + L2\n",
        "   * otimiza com SGD/Adam\n",
        "\n",
        "2. **‚ÄúFake SVM‚Äù com BCE**\n",
        "\n",
        "   * manter estrutura de SVM mas usar loss de rede neural\n",
        "\n",
        "Como voc√™ mencionou **‚Äútermo de regulariza√ß√£o‚Äù e ‚Äúperda de classifica√ß√£o‚Äù**, acho que voc√™ quer ver a **vers√£o com hinge loss mesmo**. Ela fica assim üëá\n",
        "\n",
        "```python\n",
        "class LinearSVM(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.zeros(n_features, 1))\n",
        "        self.b = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # retorna o score (n√£o √© probabilidade)\n",
        "        return x @ self.w + self.b  # shape: [N, 1]\n",
        "\n",
        "\n",
        "def hinge_loss(outputs, y, C=1.0):\n",
        "    # y: [N] com valores em {-1, +1}\n",
        "    # outputs: [N, 1]\n",
        "    # hinge: max(0, 1 - y * f(x))\n",
        "    y = y.view(-1, 1)\n",
        "    classification_loss = torch.clamp(1 - y * outputs, min=0).mean()\n",
        "    # termo de regulariza√ß√£o (1/2 ||w||^2)\n",
        "    return 0.5 * (model.w ** 2).sum() + C * classification_loss\n",
        "```\n",
        "\n",
        "Se quiser, j√° te monto o loop de treino completo para essa SVM em PyTorch com print de loss. Quer que eu fa√ßa na sequ√™ncia?\n"
      ],
      "metadata": {
        "id": "FE-cw3uwnV-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# 1. Carregar e preparar dados (Iris, 2 classes)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# manter s√≥ setosa (0) e virginica (2)\n",
        "mask = (y != 1)\n",
        "X = X[mask]\n",
        "y = y[mask]         # agora s√≥ 0 e 2\n",
        "\n",
        "# converter r√≥tulos para {-1, +1}\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "y[y == 0] = -1.0\n",
        "y[y == 2] = 1.0\n",
        "\n",
        "# 2. train/test split + normaliza√ß√£o (muito importante p/ SVM)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y.numpy(), test_size=0.2, random_state=42, shuffle=True, stratify=y.numpy()\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# converter para tensores\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.float32)\n",
        "\n",
        "# 3. Definir modelo SVM linear\n",
        "class LinearSVM(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        # w: [n_features, 1], b: [1]\n",
        "        self.w = nn.Parameter(torch.zeros(n_features, 1))\n",
        "        self.b = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [N, d]\n",
        "        # return: [N, 1] ‚Äî score (n√£o √© probabilidade)\n",
        "        return x @ self.w + self.b\n",
        "\n",
        "model = LinearSVM(n_features=4)\n",
        "\n",
        "# 4. Definir hinge loss + regulariza√ß√£o\n",
        "def svm_loss(model, outputs, y, C=1.0):\n",
        "    # outputs: [N, 1], y: [N]\n",
        "    y = y.view(-1, 1)  # [N, 1]\n",
        "    # hinge: max(0, 1 - y * f(x))\n",
        "    hinge = torch.clamp(1 - y * outputs, min=0)  # [N, 1]\n",
        "    classification_loss = hinge.mean()\n",
        "    # regulariza√ß√£o L2: 1/2 ||w||^2\n",
        "    reg = 0.5 * torch.sum(model.w ** 2)\n",
        "    # total\n",
        "    return reg + C * classification_loss, classification_loss, reg\n",
        "\n",
        "# 5. Otimizador\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 6. Treino\n",
        "EPOCHS = 1000\n",
        "C = 1.0  # quanto maior, mais o modelo tenta acertar o treino (menos margem)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_t)   # [N, 1]\n",
        "    loss, cls_loss, reg_loss = svm_loss(model, outputs, y_train_t, C=C)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0 or epoch == 1:\n",
        "        # acur√°cia de treino\n",
        "        with torch.no_grad():\n",
        "            preds_train = torch.sign(model(X_train_t)).view(-1)  # -1 ou +1\n",
        "            acc_train = (preds_train == y_train_t).float().mean().item()\n",
        "        print(f\"[{epoch:4d}/{EPOCHS}] loss={loss.item():.4f}  cls={cls_loss.item():.4f}  reg={reg_loss.item():.4f}  acc_train={acc_train:.3f}\")\n",
        "\n",
        "# 7. Avalia√ß√£o no TESTE\n",
        "with torch.no_grad():\n",
        "    scores_test = model(X_test_t).view(-1)\n",
        "    preds_test = torch.sign(scores_test)  # -1 ou +1\n",
        "    acc_test = (preds_test == y_test_t).float().mean().item()\n",
        "\n",
        "print(\"\\n=== Avalia√ß√£o no TESTE ===\")\n",
        "print(f\"Acur√°cia: {acc_test:.3f}\")\n",
        "\n",
        "# matriz de confus√£o (convertendo p/ {0,1} s√≥ pra visualizar melhor)\n",
        "y_test_bin = (y_test_t == 1).int().numpy()\n",
        "preds_test_bin = (preds_test == 1).int().numpy()\n",
        "cm = confusion_matrix(y_test_bin, preds_test_bin)\n",
        "print(\"Matriz de confus√£o:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "w7iSPpC4nYkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isso ficou lindo üòé\n",
        "\n",
        "Vamos dissecar o que o seu output est√° dizendo, porque ele est√° mostrando uma **SVM linear funcionando exatamente como no papel**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. O treino\n",
        "\n",
        "```text\n",
        "[   1/1000] loss=1.0000  cls=1.0000  reg=0.0000  acc_train=1.000\n",
        "[ 100/1000] loss=0.2398  cls=0.0585  reg=0.1813  acc_train=1.000\n",
        "...\n",
        "[1000/1000] loss=0.2384  cls=0.0508  reg=0.1876  acc_train=1.000\n",
        "```\n",
        "\n",
        "O que isso conta:\n",
        "\n",
        "* **na √©poca 1**:\n",
        "\n",
        "  * `cls=1.0000`: v√°rios pontos ainda estavam ‚Äúdentro‚Äù da margem (ou no limite) ‚Üí hinge loss alta.\n",
        "  * `reg=0.0000`: os pesos come√ßaram em zero.\n",
        "  * `acc_train=1.000`: mesmo com hinge alta, **todos j√° estavam do lado certo do hiperplano**. Isso acontece porque o Iris (s√≥ setosa vs virginica) √© **muito bem separ√°vel**.\n",
        "\n",
        "* **a partir da √©poca 100**:\n",
        "\n",
        "  * `cls` (perda de classifica√ß√£o) caiu para ~0.05 ‚Üí ou seja, quase todo mundo ficou **fora da margem**.\n",
        "  * `reg` subiu para ~0.187 ‚Üí o modelo ‚Äúesticou‚Äù um pouco o vetor (w) para **aumentar a margem**, e a regulariza√ß√£o come√ßou a pesar.\n",
        "  * `loss` total estabilizou em **0.2384** ‚Üí isso √© o equil√≠brio cl√°ssico da SVM:\n",
        "    [\n",
        "    \\text{loss total} = \\underbrace{0.5|w|^2}*{\\text{regulariza√ß√£o}} + C \\cdot \\underbrace{\\text{hinge}}*{\\text{classifica√ß√£o}}\n",
        "    ]\n",
        "\n",
        "üìå O que isso mostra?\n",
        "Que o modelo **chegou muito r√°pido a 100% de acerto** e depois ficou **apenas ajustando a margem** ‚Äî exatamente o comportamento esperado de uma SVM quando os dados s√£o **linearmente separ√°veis**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Treino com acur√°cia 1.000 desde o come√ßo\n",
        "\n",
        "Isso pode parecer estranho (‚Äúu√©, j√° acertou tudo na √©poca 1?‚Äù), mas faz sentido:\n",
        "\n",
        "* voc√™ usou **s√≥ duas classes bem separadas do Iris** (setosa e virginica);\n",
        "* voc√™ **normalizou** os dados;\n",
        "* o modelo √© **linear**, e o problema √© **linearmente separ√°vel**.\n",
        "\n",
        "Ent√£o, o que a SVM faz depois disso n√£o √© mais ‚Äúaprender a classificar‚Äù, e sim:\n",
        "\n",
        "> ‚Äúok, j√° separei ‚Äî agora vou achar **a melhor separa√ß√£o poss√≠vel** (a de maior margem)‚Äù.\n",
        "\n",
        "√â por isso que:\n",
        "\n",
        "* `acc_train` ficou 1.000 o tempo todo ‚úÖ\n",
        "* mas `cls` e `reg` ficaram se ajustando at√© estabilizar\n",
        "\n",
        "Ou seja:\n",
        "**classifica√ß√£o perfeita** + **busca da margem √≥tima**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Avalia√ß√£o no teste\n",
        "\n",
        "```text\n",
        "=== Avalia√ß√£o no TESTE ===\n",
        "Acur√°cia: 1.000\n",
        "Matriz de confus√£o:\n",
        " [[10  0]\n",
        " [ 0 10]]\n",
        "```\n",
        "\n",
        "Isso aqui √© o cen√°rio ideal:\n",
        "\n",
        "* 10 negativos (setosa) ‚Üí todos corretos\n",
        "* 10 positivos (virginica) ‚Üí todos corretos\n",
        "* nenhuma confus√£o, nenhum falso positivo, nenhum falso negativo\n",
        "\n",
        "üëâ **Sua SVM generalizou perfeitamente.**\n",
        "\n",
        "Isso confirma duas coisas importantes:\n",
        "\n",
        "1. **O dataset nesse recorte (0 vs 2) √© mesmo linearmente separ√°vel.**\n",
        "2. **A implementa√ß√£o da SVM em PyTorch est√° correta.**\n",
        "   (se tivesse algum erro de sinal, shape, ou r√≥tulo {0,1} vs {-1,+1}, voc√™ veria 1 ou 2 erros aqui)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Papel dos dois termos (ficou vis√≠vel!)\n",
        "\n",
        "Olha que legal:\n",
        "\n",
        "* `cls ‚âà 0.05`\n",
        "  ‚Üí quase ningu√©m mais est√° violando a margem\n",
        "  ‚Üí **perda de classifica√ß√£o baixa**\n",
        "\n",
        "* `reg ‚âà 0.187`\n",
        "  ‚Üí o modelo est√° pagando um custo por ter um (w) com certo tamanho\n",
        "  ‚Üí **isso √© o termo de regulariza√ß√£o empurrando a solu√ß√£o para uma margem m√°xima**\n",
        "\n",
        "Ou seja: voc√™ viu **ao vivo** o trade-off que a gente comentou:\n",
        "\n",
        "[\n",
        "\\text{Loss total} = \\underbrace{0.5|w|^2}*{\\text{regulariza√ß√£o}} + C \\cdot \\underbrace{\\text{hinge}}*{\\text{classifica√ß√£o}}\n",
        "]\n",
        "\n",
        "Se voc√™ agora **aumentar o C** (ex.: `C = 10` no `svm_loss`), o que vai acontecer?\n",
        "\n",
        "* o modelo vai tentar zerar ainda mais o `cls`\n",
        "* mas o `reg` vai subir ‚Üí margem fica menor\n",
        "* em dados com ru√≠do, isso levaria a overfitting\n",
        "\n",
        "Se voc√™ **diminuir o C** (ex.: `C = 0.1`):\n",
        "\n",
        "* o modelo aceita um pouco mais de viola√ß√£o de margem (`cls` sobe)\n",
        "* mas o peso d√° uma encolhida (`reg` desce)\n",
        "* voc√™ ganha **margem maior** ‚Üí mais robusto\n",
        "\n",
        "D√° pra testar isso direto no seu c√≥digo trocando s√≥ o `C` üëç\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Ent√£o‚Ä¶ esse √© um bom modelo preditivo?\n",
        "\n",
        "**Sim.** Para esse recorte do problema (Iris 0 vs 2), ele est√° **perfeito**.\n",
        "\n",
        "Mas √© importante dizer:\n",
        "\n",
        "* voc√™ escolheu **um problema f√°cil** (linearmente separ√°vel);\n",
        "* em problemas reais, com ru√≠do, 3+ classes, ou dados n√£o lineares, a SVM **n√£o vai ficar 1.000 / 1.000 assim**;\n",
        "* a√≠ entra: kernel, modelo neural ou features melhores.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. O que isso te mostra na aula de LLMs\n",
        "\n",
        "Isso que voc√™ acabou de fazer mostra **a ponte**:\n",
        "\n",
        "* SVM ‚Üí separa vetores por produto interno + regulariza√ß√£o\n",
        "* LLM ‚Üí tamb√©m compara vetores por produto interno (Q¬∑K·µÄ), mas **aprende o espa√ßo de vetores**\n",
        "* GPU ‚Üí m√°quina de produto interno\n",
        "\n",
        "Voc√™ acabou de ver o caso ‚Äúpuro‚Äù do aprendizado estat√≠stico acontecendo.\n",
        "Agora d√° pra mostrar em aula: ‚Äúolha, a mesma ideia de separar no espa√ßo vetorial √© o que o Transformer faz, mas com aten√ß√£o e embeddings‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "Se quiser, o pr√≥ximo passo natural √©:\n",
        "\n",
        "1. repetir o mesmo c√≥digo **sem normalizar** e ver que a loss n√£o fica t√£o bonitinha;\n",
        "2. ou trocar pra **kernel RBF** no sklearn e comparar;\n",
        "3. ou remontar o mesmo problema mas **com as 3 classes** e mostrar por que a SVM precisa do truque ‚Äúone-vs-rest‚Äù.\n",
        "\n",
        "Quer ir pra qual desses? üëÄ\n",
        "\n"
      ],
      "metadata": {
        "id": "u2XuJ05kpeKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir par√¢metros trein√°veis da Support Vector Machine: w e b\n",
        "n_features = X.shape[1]\n",
        "w = torch.randn(n_features, 1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# === Hiperpar√¢metros ===\n",
        "learning_rate = 0.01\n",
        "epochs = 300\n",
        "optimizer = optim.Adam([w, b], lr=learning_rate)"
      ],
      "metadata": {
        "id": "eg97DxIbe0tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfeito üëè ‚Äî esse trecho que voc√™ acabou de colocar est√° montando a vers√£o ‚Äúmanual‚Äù dos par√¢metros da SVM, e √© uma √≥tima forma de entender como ela funciona **por dentro**.\n",
        "\n",
        "Vamos destrinchar o que voc√™ fez e o que viria a seguir üëá\n",
        "\n",
        "---\n",
        "\n",
        "## üß† O que o c√≥digo faz\n",
        "\n",
        "```python\n",
        "n_features = X.shape[1]\n",
        "w = torch.randn(n_features, 1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "```\n",
        "\n",
        "### üîπ `w`\n",
        "\n",
        "* √â o **vetor de pesos** da SVM (a normal do hiperplano).\n",
        "* Tem dimens√£o `(n_features, 1)` ‚Äî ou seja, um peso para cada feature do Iris.\n",
        "* Inicializado aleatoriamente com distribui√ß√£o normal.\n",
        "\n",
        "### üîπ `b`\n",
        "\n",
        "* √â o **bias** (termo de deslocamento).\n",
        "* Inicializado como zero.\n",
        "* Tamb√©m tem gradiente habilitado ‚Üí vai ser ajustado durante o treinamento.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Hiperpar√¢metros\n",
        "\n",
        "```python\n",
        "learning_rate = 0.01\n",
        "epochs = 300\n",
        "optimizer = optim.Adam([w, b], lr=learning_rate)\n",
        "```\n",
        "\n",
        "### üîπ `learning_rate`\n",
        "\n",
        "* Controla o passo de atualiza√ß√£o dos par√¢metros.\n",
        "* O valor 0.01 √© razo√°vel para Adam.\n",
        "\n",
        "### üîπ `epochs`\n",
        "\n",
        "* N√∫mero de vezes que o modelo vai ver o conjunto de treino inteiro.\n",
        "\n",
        "### üîπ `optimizer`\n",
        "\n",
        "* Adam vai atualizar diretamente `w` e `b`.\n",
        "* Isso √© legal porque **voc√™ n√£o est√° usando um `nn.Module`**, ent√£o precisa passar os par√¢metros manualmente.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Pr√≥ximo passo ‚Äî definir a **fun√ß√£o de perda da SVM**\n",
        "\n",
        "Agora voc√™ precisa criar a **hinge loss com regulariza√ß√£o**, igual √† que usamos antes com a classe `LinearSVM`:\n",
        "\n",
        "```python\n",
        "def svm_loss(X, y, w, b, C=1.0):\n",
        "    # Forward: f(x) = Xw + b\n",
        "    outputs = X @ w + b\n",
        "\n",
        "    # Hinge loss: max(0, 1 - y * f(x))\n",
        "    hinge = torch.clamp(1 - y.view(-1, 1) * outputs, min=0)\n",
        "    classification_loss = hinge.mean()\n",
        "\n",
        "    # Regulariza√ß√£o L2: 1/2 ||w||^2\n",
        "    reg = 0.5 * torch.sum(w ** 2)\n",
        "\n",
        "    return reg + C * classification_loss, classification_loss, reg\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Loop de treinamento\n",
        "\n",
        "Voc√™ pode usar o mesmo padr√£o de loop:\n",
        "\n",
        "```python\n",
        "C = 1.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss, cls_loss, reg_loss = svm_loss(X, y, w, b, C=C)\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == 1:\n",
        "        print(f\"[{epoch:4d}/{epochs}] loss={total_loss.item():.4f}  cls={cls_loss.item():.4f}  reg={reg_loss.item():.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Avalia√ß√£o final\n",
        "\n",
        "Ap√≥s o treino:\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    preds = torch.sign(X @ w + b)\n",
        "    acc = (preds.view(-1) == y).float().mean()\n",
        "    print(f\"Acur√°cia final: {acc:.3f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Comparando com o modelo anterior\n",
        "\n",
        "Esse c√≥digo √© equivalente ao que voc√™ fez antes com `nn.Module`, mas aqui:\n",
        "\n",
        "* tudo √© expl√≠cito (voc√™ v√™ exatamente os tensores `w` e `b`);\n",
        "* o otimizador atualiza diretamente esses tensores;\n",
        "* voc√™ entende claramente onde est√° cada termo:\n",
        "  regulariza√ß√£o (`0.5 * ||w||¬≤`) e hinge (`max(0, 1 - y f(x))`).\n",
        "\n",
        "Se voc√™ rodar, deve ver a **mesma converg√™ncia r√°pida** (loss estabilizando ~0.23 e acur√°cia = 1.000).\n",
        "\n",
        "---\n",
        "\n",
        "Quer que eu adicione **a parte de divis√£o treino/teste e normaliza√ß√£o** dentro desse c√≥digo para deixar ele completinho e compar√°vel com o modelo anterior?\n"
      ],
      "metadata": {
        "id": "RyXXXFK5q2fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# 1) Carregar dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data      # shape (150, 4)\n",
        "y = iris.target    # 0=setosa, 1=versicolor, 2=virginica\n",
        "\n",
        "# 2) Ficar s√≥ com 2 classes: 0 e 2 (tirar 1)\n",
        "mask = (y != 1)\n",
        "X = X[mask]\n",
        "y = y[mask]   # agora tem s√≥ 0 e 2\n",
        "\n",
        "# 3) Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
        ")\n",
        "\n",
        "# 4) Normalizar (fit no treino, transform no teste)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# 5) Converter para tensores e trocar r√≥tulos para {-1, +1}\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
        "\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.float32)\n",
        "\n",
        "# SVM usa -1 e +1\n",
        "y_train_t[y_train_t == 0] = -1.0\n",
        "y_train_t[y_train_t == 2] =  1.0\n",
        "y_test_t[y_test_t == 0]   = -1.0\n",
        "y_test_t[y_test_t == 2]   =  1.0\n",
        "\n",
        "# 6) Definir par√¢metros trein√°veis (w e b) \"na m√£o\"\n",
        "n_features = X_train_t.shape[1]\n",
        "w = torch.randn(n_features, 1, requires_grad=True)  # pesos\n",
        "b = torch.zeros(1, requires_grad=True)              # bias\n",
        "\n",
        "# 7) Hiperpar√¢metros\n",
        "learning_rate = 0.01\n",
        "epochs = 300\n",
        "C = 1.0   # peso da perda de classifica√ß√£o (como o C da SVM)\n",
        "optimizer = optim.Adam([w, b], lr=learning_rate)\n",
        "\n",
        "# 8) Definir a loss da SVM (hinge + regulariza√ß√£o)\n",
        "def svm_loss(X, y, w, b, C=1.0):\n",
        "    # X: [N, d], y: [N], w: [d, 1], b: [1]\n",
        "    scores = X @ w + b          # [N, 1]\n",
        "    y = y.view(-1, 1)           # [N, 1]\n",
        "    # hinge: max(0, 1 - y * score)\n",
        "    hinge = torch.clamp(1 - y * scores, min=0)\n",
        "    classification_loss = hinge.mean()\n",
        "    reg_loss = 0.5 * torch.sum(w ** 2)\n",
        "    total = reg_loss + C * classification_loss\n",
        "    return total, classification_loss, reg_loss, scores\n",
        "\n",
        "# 9) Treino\n",
        "for epoch in range(1, epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "    total_loss, cls_loss, reg_loss, scores = svm_loss(X_train_t, y_train_t, w, b, C=C)\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == 1:\n",
        "        # acur√°cia de treino\n",
        "        with torch.no_grad():\n",
        "            preds_train = torch.sign(scores).view(-1)\n",
        "            acc_train = (preds_train == y_train_t).float().mean().item()\n",
        "        print(f\"[{epoch:3d}/{epochs}] loss={total_loss.item():.4f}  cls={cls_loss.item():.4f}  reg={reg_loss.item():.4f}  acc_train={acc_train:.3f}\")\n",
        "\n",
        "# 10) Avalia√ß√£o no TESTE\n",
        "with torch.no_grad():\n",
        "    test_scores = X_test_t @ w + b\n",
        "    preds_test = torch.sign(test_scores).view(-1)\n",
        "\n",
        "acc_test = (preds_test == y_test_t).float().mean().item()\n",
        "print(\"\\n=== Avalia√ß√£o no TESTE ===\")\n",
        "print(f\"Acur√°cia: {acc_test:.3f}\")\n",
        "\n",
        "# matriz de confus√£o (convertendo p/ {0,1} s√≥ pra ficar bonita)\n",
        "y_test_bin = (y_test_t == 1).int().numpy()\n",
        "preds_test_bin = (preds_test == 1).int().numpy()\n",
        "cm = confusion_matrix(y_test_bin, preds_test_bin)\n",
        "print(\"Matriz de confus√£o:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "F4vXgfQmrXj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execu√ß√£o do treinamento"
      ],
      "metadata": {
        "id": "Agjn3aQxfHOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = X @ w + b  # Modelo SVM (um hiperplano que depende de w e b)\n",
        "\n",
        "    # Hinge loss: max(0, 1 - y_i * (w^T x_i + b))\n",
        "    perda_de_classificacao = torch.clamp(1 - y.view(-1, 1) * y_pred, min=0).mean()\n",
        "\n",
        "    # Termo de regulariza√ß√£o\n",
        "    perda_de_distancia_entre_classes = 0.5 * torch.sum(w ** 2) # 2/||w|| √© a dist√¢ncia que queremos que seja a maior poss√≠vel\n",
        "\n",
        "    # Fun√ß√£o objetivo tradicional: minimizar reg + C * hinge\n",
        "    loss = perda_de_distancia_entre_classes + perda_de_classificacao\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss={loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "uuksjyq7e4Mt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}