{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdinandrafols/IA_LLMs/blob/main/aula02_tokenizacao_pratica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 2 - Tokeniza√ß√£o"
      ],
      "metadata": {
        "id": "R8ElgxbDEATe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 - Pr√©-tokeniza√ß√£o\n"
      ],
      "metadata": {
        "id": "2ZRQpsLhIGNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers\n",
        "\n",
        "tok_ws = Tokenizer(models.BPE())\n",
        "tok_ws.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "frase = \"N√£o, ser√° punido o criminoso.\"\n",
        "\n",
        "print(tok_ws.pre_tokenizer.pre_tokenize_str(frase))\n"
      ],
      "metadata": {
        "id": "HKHYbj5-FU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Punctuation + Whitespace*"
      ],
      "metadata": {
        "id": "XCj0m4asF14E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_punc = Tokenizer(models.BPE())\n",
        "tok_punc.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),\n",
        "    pre_tokenizers.Punctuation()\n",
        "])\n",
        "print(tok_punc.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "GEhKhK5UFoZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretokenizer: ByteLevel - estilo GPT-2**"
      ],
      "metadata": {
        "id": "TE4jFk36Gw2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_byte = Tokenizer(models.BPE())\n",
        "tok_byte.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "print(tok_byte.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "Q9AANXjUF8wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metaspace (SentencePiece style)"
      ],
      "metadata": {
        "id": "MrgFHmqvG3tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_meta = Tokenizer(models.BPE())\n",
        "tok_meta.pre_tokenizer = pre_tokenizers.Metaspace()\n",
        "print(tok_meta.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "h_5CuANPG55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento\n"
      ],
      "metadata": {
        "id": "XH9EDxlNH5PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 02_tokenizer_train.ipynb\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "# 1. Vis√£o geral do algoritmo BPE\n",
        "print(\"Treinar o tokenizador (BPE):\\n\")\n",
        "print(\"1. Comece com todos os caracteres presentes no corpus como tokens.\")\n",
        "print(\"2. Encontre e una o par de tokens mais frequente em um novo token.\")\n",
        "print(\"3. Repita at√© atingir o tamanho de vocabul√°rio desejado.\\n\")\n",
        "\n",
        "# 2. Corpus de treino\n",
        "corpus = [\"Hello, world!\", \"Hello there\", \"World of BPE\"]\n",
        "print(\"Corpus de treino:\", corpus, \"\\n\")\n",
        "\n",
        "# 3. Configura√ß√£o do tokenizador\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "bpe_trainer = trainers.BpeTrainer(\n",
        "    vocab_size=50,\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
        ")\n",
        "\n",
        "# 4. Treinamento\n",
        "tokenizer.train_from_iterator(corpus, trainer=bpe_trainer)\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(f\"Tamanho do vocabul√°rio: {len(vocab)}\\n\")\n",
        "\n",
        "# 5. Visualizando parte do vocabul√°rio\n",
        "sorted_vocab = sorted(vocab.items(), key=lambda kv: kv[1])[:20]\n",
        "for token, idx in sorted_vocab:\n",
        "    print(f\"{idx:>3} ‚Üí {repr(token)}\")\n",
        "\n",
        "# 6. Salvando e recarregando\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "tokenizer_new = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# 7. Testando em novas frases\n",
        "textos = [\"O rato roeu a roupa do rei de Roma\", \"Hello, world.\"]\n",
        "\n",
        "print(\"\\nTokeniza√ß√£o de exemplos:\")\n",
        "for texto in textos:\n",
        "    out = tokenizer_new.encode(texto)\n",
        "    print(f\"Texto: {texto}\")\n",
        "    print(f\"Tokens: {out.tokens}\")\n",
        "    print(f\"IDs: {out.ids}\\n\")\n"
      ],
      "metadata": {
        "id": "3KIL27TjILDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode"
      ],
      "metadata": {
        "id": "3bkj5aXXJ3xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_tokenizer_encode.ipynb\n",
        "# Pipeline de tokeniza√ß√£o: normaliza√ß√£o ‚Üí pr√©-tokeniza√ß√£o ‚Üí modelo ‚Üí p√≥s-processamento\n",
        "\n",
        "from tokenizers import Tokenizer, normalizers, pre_tokenizers, processors\n",
        "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace, Digits, Sequence\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "print(\"### Pipeline de tokeniza√ß√£o ###\")\n",
        "print(\" Normalization\")\n",
        "print(\" Pre-tokenization\")\n",
        "print(\" Model\")\n",
        "print(\" Post-processing\\n\")\n",
        "\n",
        "# Carregar o tokenizador treinado (BPE)\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Normalization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Normalization\")\n",
        "normalizer = normalizers.Sequence([\n",
        "    NFD(),          # decomposi√ß√£o de acentos\n",
        "    Lowercase(),    # tudo min√∫sculo\n",
        "    StripAccents()  # remove acentos\n",
        "])\n",
        "texto = \"H√©ll√≤ h√¥w are √º?\"\n",
        "print(\"Antes:\", texto)\n",
        "print(\"Depois:\", normalizer.normalize_str(texto), \"\\n\")\n",
        "tokenizer.normalizer = normalizer\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Pre-tokenization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Pre-tokenization\")\n",
        "pre_tok = Sequence([\n",
        "    Whitespace(),\n",
        "    Digits(individual_digits=True)\n",
        "])\n",
        "texto2 = \"Hello! How are you? Tenho R$ 213,12.\"\n",
        "print(\"Pr√©-tokeniza√ß√£o:\", pre_tok.pre_tokenize_str(texto2), \"\\n\")\n",
        "tokenizer.pre_tokenizer = pre_tok\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Model\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Model: BPE (Byte Pair Encoding)\")\n",
        "# j√° carregado do arquivo bpe_tokenizer.json\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Post-processing\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Post-processing (TemplateProcessing)\")\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Aplicando tudo\n",
        "# -----------------------------------------------------------\n",
        "encoded = tokenizer.encode(\"ol√° mundo\")\n",
        "print(\"Tokens IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)\n"
      ],
      "metadata": {
        "id": "WLRBw0K_J1_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bytelevel vs SentencePiece"
      ],
      "metadata": {
        "id": "_XKY1lw2NuNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_bytelevel_vs_sentencepiece.ipynb\n",
        "# Comparando ByteLevel (GPT-2) vs SentencePiece (mT5)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import unicodedata\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Modelos\n",
        "# -----------------------------\n",
        "BYTELEVEL_MODEL = \"openai-community/gpt2\"\n",
        "SENTPIECE_MODEL = \"google/mt5-small\"\n",
        "\n",
        "tok_byte = AutoTokenizer.from_pretrained(BYTELEVEL_MODEL)\n",
        "tok_spm  = AutoTokenizer.from_pretrained(SENTPIECE_MODEL)\n",
        "\n",
        "# Garantir pad_token\n",
        "if tok_byte.pad_token is None and hasattr(tok_byte, \"eos_token\"):\n",
        "    tok_byte.pad_token = tok_byte.eos_token\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Texto de exemplo\n",
        "# -----------------------------\n",
        "text = \"Vamos comer, vov√≥! üôÇ\"\n",
        "print(f\"Texto: {text}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Tokeniza√ß√£o\n",
        "# -----------------------------\n",
        "def encode_details(tokenizer, name):\n",
        "    enc = tokenizer(text, add_special_tokens=True, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    ids = enc[\"input_ids\"]\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"IDs:\", ids)\n",
        "    print(\"Qtd tokens:\", len(tokens))\n",
        "    print(\"Decoded:\", tokenizer.decode(ids))\n",
        "    print(\"Offsets:\", offsets)\n",
        "    print()\n",
        "\n",
        "encode_details(tok_byte, \"ByteLevel (GPT-2)\")\n",
        "encode_details(tok_spm, \"SentencePiece (mT5)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Compara√ß√£o Unicode (opcional)\n",
        "# -----------------------------\n",
        "def show_unicode_chars(s):\n",
        "    for ch in s:\n",
        "        name = unicodedata.name(ch, \"UNKNOWN\")\n",
        "        print(f\"{repr(ch)} -> {name}\")\n",
        "\n",
        "print(\"\\nCaracteres Unicode do texto:\")\n",
        "show_unicode_chars(text)\n"
      ],
      "metadata": {
        "id": "mhEG4EpoLiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avalia√ß√£o"
      ],
      "metadata": {
        "id": "cj23lfrwPLjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Carrega o tokenizador treinado\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# Corpus de teste (pode ser parte do seu corpus real)\n",
        "test_texts = [\n",
        "    \"O rato roeu a roupa do rei de Roma.\",\n",
        "    \"Aprender tokeniza√ß√£o √© divertido!\",\n",
        "    \"GPT-2 e mT5 usam abordagens diferentes.\",\n",
        "    \"Python √© √≥timo para NLP üòÑ\",\n",
        "]\n",
        "\n",
        "# Fun√ß√µes auxiliares\n",
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def evaluate_tokenizer(tokenizer, texts):\n",
        "    stats = []\n",
        "    for t in texts:\n",
        "        enc = tokenizer.encode(t)\n",
        "        stats.append({\n",
        "            \"text\": t,\n",
        "            \"chars\": count_chars(t),\n",
        "            \"words\": count_words(t),\n",
        "            \"tokens\": len(enc.tokens),\n",
        "            \"unk\": enc.tokens.count(\"<unk>\"),\n",
        "            \"decoded_ok\": (tokenizer.decode(enc.ids) == t)\n",
        "        })\n",
        "    return stats\n",
        "\n",
        "stats = evaluate_tokenizer(tokenizer, test_texts)\n",
        "\n",
        "# Converter para m√©tricas agregadas\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(stats)\n",
        "\n",
        "tpc = (df[\"tokens\"] / df[\"chars\"]).mean()\n",
        "tpw = (df[\"tokens\"] / df[\"words\"]).mean()\n",
        "unk_rate = (df[\"unk\"].sum() / df[\"tokens\"].sum()) * 100\n",
        "decode_acc = (df[\"decoded_ok\"].mean()) * 100\n",
        "\n",
        "print(\"=== M√©tricas de efici√™ncia ===\")\n",
        "print(f\"Tokens por caractere (TPC): {tpc:.3f}\")\n",
        "print(f\"Tokens por palavra (TPW): {tpw:.3f}\")\n",
        "print(f\"Percentual de <unk>: {unk_rate:.2f}%\")\n",
        "print(f\"Reversibilidade (decode == original): {decode_acc:.1f}%\")\n",
        "print(f\"Tamanho m√©dio da sequ√™ncia: {df['tokens'].mean():.1f} tokens/frase\")\n"
      ],
      "metadata": {
        "id": "hfiIxoW5PN6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}